{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJSKz7HIwkFqrXr2J5yccJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2840bongkoch/BSC_DPDM2025/blob/main/midterm_663020284_0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d1e09e8",
        "outputId": "1b5fdf65-338c-4981-ba10-59019efe78da"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "นางสาวบงกช เจตวัน 663020284-0 SC-SI"
      ],
      "metadata": {
        "id": "tRtQRg4O51lM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ลิ้งส่งงาน\n",
        "ไฟล์ผลลัพธ์ข้อมูล csv [ไฟล์ผลลัพธ์ข้อมูล csv](https://drive.google.com/file/d/1O8TZpu5CCYygXzWYLv0p8LakEW8eGAbN/view?usp=sharing)\n",
        "\n",
        "บันทึกการใช้ AI [บันทึกการใช้ AI](https://kkumail-my.sharepoint.com/personal/bongkoch_j_kkumail_com/Documents/%E0%B8%9A%E0%B8%B1%E0%B8%99%E0%B8%97%E0%B8%B6%E0%B8%81%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B9%80%E0%B8%AD%E0%B9%84%E0%B8%AD.pdf)"
      ],
      "metadata": {
        "id": "8cg2_Rw13lOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "โจทย์นี้เป็นงานจริงของสถาบันสารสนเทศและทรัพยกรน้ำ สามารใช้ AI ในหารคำนวณ แต่ต้อง\n",
        "1. บันทึก prompt ทุกครั้ง\n",
        "2. อธิบายขั้นตอนอย่างละเอียด\n",
        "3. อธิบายการทำงานของโค้ดทุกบรรทัก\n",
        "4. ตรวจสอบความถูกต้องของข้อมูล\n",
        "\n",
        "หมายหตุ ข้อมูลสูญหาย คือ record ที่ไม่มีในไฟล์เลย (ไม่ใช่ค่า null หรือ Null)"
      ],
      "metadata": {
        "id": "3DFZREuqT3M9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ไฟล์\n",
        "1. historical_daily_rainfall.csv เป็นข้อมูลปริมาณน้ำฝนรายวันย้อนหลัง (สำหรับคำนวณค่าสถิติ)\n",
        "2. โฟลเดอร์ Input-(Operation Daily rain from API_2025) เป็นข้อมูลปริมาณน้ำฝนรายวะนที่ต้องการนำมาประมวลผล\n",
        "3. Stations in the HII.csv รายชื่อสถานีใช้งานได้\n",
        "4. output_montly_rainfall.csv เป็นผลลัพทธ์ของข้อมูลที่ต้องการ"
      ],
      "metadata": {
        "id": "9JQPxFA1Xa_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "วิธีการทำ\n",
        "1. กรองสถานีเฉพาะสถานทีที่อยู่ใน Stations in the model_Hll.csv\n",
        "2. ลบสถานทีที่มีข้อมูลสูญหาย มากกว่าเท่ากับ 20%\n",
        "3. เติมข้อมูลด้วยค่า median ของ Day of Year จาก histotical\n",
        "4. รวมปริมาณฝนแต่ละวันเป็นผลรวมรายเดือน"
      ],
      "metadata": {
        "id": "BCmbGlg4Uisl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "คำถาม\n",
        "1. อ่านไฟล์ทั้งหมด แสดงโครงสร้างข้อมูล และอธิบายความหมายของแต่ละคอลัมน์\n",
        "2. กรองข้อมูลให้เหลือเฉพาะสถานีใน Stations in  the model_Hll.csv แสดงจำนวนสถานีก่อน/หลังกรอง\n",
        "3. ระบุข้อมูลสูญหาย (record ที่ห่ยไป) ของแต่ละสถานี แสดงจำนวนและสัดส่วน\n",
        "4. คำสถานีที่มีข้อมูลสูญหาย มากกว่าเท่ากับ 20% แสดงรายชื่อสถานีที่ถูกคัดออกและจำนวนสถานีที่เหลือ\n",
        "5. เติมข้อมูลหายไปด้วยค่า median ตาม Day of Year จาก histotical แยกตามสถานี ตรวจสอบว่าไม่มี record หายอีก\n",
        "6. อธิบายเหตุผลที่ใช้ median แทน mean\n",
        "7. คำนวณผลรวมปริมาณน้ำฝนรายเดือน จัดรูปแบบให้ตรงกับ output_montly_rainfall.csv"
      ],
      "metadata": {
        "id": "inArLgfRVJbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สิ่งที่ต้องส่ง\n",
        "1. ไฟล์ Calab Notebook (.ipynb) ที่มีโ้ดและคำอธิบายครบถ้วน\n",
        "2. ไฟลผลลัพธ์ข้อมูลรายเดือน (.csv) link Google drive ของไฟล์ output เปิดเป็นสาธารณะ\n",
        "3. บันทึกการใช้ AI (prompt, ผลลัพทธ์, การปรับแก้, เหตุผล)  Stations in  the model_Hll"
      ],
      "metadata": {
        "id": "NzK8higCWM2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stations = pd.read_csv('/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/Stations in the model_HII.csv')\n",
        "stations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MrDZmEWDYSMl",
        "outputId": "b841f551-1e56-4f39-8281-6de4142d48ba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Node        Lat        Long\n",
              "0      ACRU  15.788054  104.642235\n",
              "1      BBHN   9.536913   98.579490\n",
              "2      BBUA  17.372976  103.984436\n",
              "3      BCNG  17.386840  103.290870\n",
              "4      BDAR  14.587073  102.495190\n",
              "..      ...        ...         ...\n",
              "380  YOM003  18.265924  100.177060\n",
              "381  YOM008  16.708166  100.171780\n",
              "382  YOM009  16.310750  100.271750\n",
              "383  YOM010  18.133717  100.124596\n",
              "384  YOM012  17.021260   99.819710\n",
              "\n",
              "[385 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cff6c26f-2698-4abc-b0bd-4b35a3096403\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Node</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACRU</td>\n",
              "      <td>15.788054</td>\n",
              "      <td>104.642235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BBHN</td>\n",
              "      <td>9.536913</td>\n",
              "      <td>98.579490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BBUA</td>\n",
              "      <td>17.372976</td>\n",
              "      <td>103.984436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BCNG</td>\n",
              "      <td>17.386840</td>\n",
              "      <td>103.290870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BDAR</td>\n",
              "      <td>14.587073</td>\n",
              "      <td>102.495190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>YOM003</td>\n",
              "      <td>18.265924</td>\n",
              "      <td>100.177060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>YOM008</td>\n",
              "      <td>16.708166</td>\n",
              "      <td>100.171780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>YOM009</td>\n",
              "      <td>16.310750</td>\n",
              "      <td>100.271750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>YOM010</td>\n",
              "      <td>18.133717</td>\n",
              "      <td>100.124596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>YOM012</td>\n",
              "      <td>17.021260</td>\n",
              "      <td>99.819710</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>385 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cff6c26f-2698-4abc-b0bd-4b35a3096403')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cff6c26f-2698-4abc-b0bd-4b35a3096403 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cff6c26f-2698-4abc-b0bd-4b35a3096403');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_5f319495-4570-4127-b3e6-53356b71bd1d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('stations')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5f319495-4570-4127-b3e6-53356b71bd1d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('stations');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "stations",
              "summary": "{\n  \"name\": \"stations\",\n  \"rows\": 385,\n  \"fields\": [\n    {\n      \"column\": \"Node\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 385,\n        \"samples\": [\n          \"SKG007\",\n          \"RAY001\",\n          \"WAN001\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5510405839736316,\n        \"min\": 5.9392657,\n        \"max\": 20.240082,\n        \"num_unique_values\": 385,\n        \"samples\": [\n          15.78322,\n          12.803011,\n          17.260607\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Long\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5563816572891251,\n        \"min\": 97.93514,\n        \"max\": 105.50172,\n        \"num_unique_values\": 385,\n        \"samples\": [\n          99.65322,\n          101.65024,\n          99.078\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_FIk7OrgYzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhnXuLKzdRYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7b48b6"
      },
      "source": [
        "# Task\n",
        "Process daily rainfall data from the Google Drive folder \"Input-(Operation Daily rain from API_2025)\", filtering stations based on \"Stations in the model_HII.csv\", identifying and handling missing records (removing stations with greater than or equal to 20% missing data, and imputing the remaining missing data using the median 'Day of Year' from \"hii_daily_rain.csv\"), then calculate the monthly rainfall totals and save the results to \"Y_montly_rain_hii_operation.csv\", providing a publicly accessible Google Drive link to the output file along with a comprehensive Colab notebook detailing all steps and explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f22deae3"
      },
      "source": [
        "## Define Google Drive Paths\n",
        "\n",
        "### Subtask:\n",
        "จัดเตรียมโค้ดเพื่อกำหนดพาธ (path) สำหรับไฟล์และโฟลเดอร์ข้อมูลทั้งหมดใน Google Drive ที่เชื่อมต่อไว้ โดยจะใช้พาธตามตัวอย่างที่คุณเคยใช้งานสำเร็จแล้ว ซึ่งประกอบด้วย 'hii_daily_rain.csv' (ซึ่งคือ historical daily rainfall), 'Stations in the model_HII.csv' และโฟลเดอร์ 'Input-(Operation Daily rain from API_2025)' เพื่อให้สามารถเข้าถึงข้อมูลได้สะดวก\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21badd24"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the paths to various files and folders in Google Drive. I will use the `os` module to construct these paths dynamically, ensuring they are correctly defined for subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bb6e794",
        "outputId": "37cd1337-ce40-4401-ef25-7f9a1d427940"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base path to your 'midterm' folder in Google Drive\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/'\n",
        "\n",
        "# Define the path for the folder containing the input daily rainfall data\n",
        "input_folder_path = os.path.join(base_path, 'Input - (Operation Daily rain from API_ 2025)')\n",
        "\n",
        "# Define the full path for the historical daily rainfall file (hii_daily_rain.csv)\n",
        "historical_daily_rainfall_file = os.path.join(input_folder_path, 'hii_daily_rain.csv')\n",
        "\n",
        "# Define the full path for the stations definition file\n",
        "stations_model_file = os.path.join(input_folder_path, 'Stations in the model_HII.csv')\n",
        "\n",
        "# Define the full path for the output monthly rainfall file\n",
        "output_monthly_rainfall_file = os.path.join(base_path, 'Y_montly_rain_hii_operation.csv')\n",
        "\n",
        "print(f\"Base Path: {base_path}\")\n",
        "print(f\"Input Folder Path: {input_folder_path}\")\n",
        "print(f\"Historical Daily Rainfall File: {historical_daily_rainfall_file}\")\n",
        "print(f\"Stations Model File: {stations_model_file}\")\n",
        "print(f\"Output Monthly Rainfall File: {output_monthly_rainfall_file}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Path: /content/drive/MyDrive/Data mining/midterm/\n",
            "Input Folder Path: /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)\n",
            "Historical Daily Rainfall File: /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/hii_daily_rain.csv\n",
            "Stations Model File: /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/Stations in the model_HII.csv\n",
            "Output Monthly Rainfall File: /content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f2ed07"
      },
      "source": [
        "## Load and Inspect Stations Data\n",
        "\n",
        "### Subtask:\n",
        "อ่านไฟล์ 'Stations in the model_HII.csv' ซึ่งเป็นรายชื่อสถานีที่ใช้งานได้ เข้ามาเก็บใน DataFrame จากนั้นแสดงผลข้อมูลเบื้องต้น (เช่น 5 แถวแรก, ชนิดข้อมูล, ค่าสถิติเบื้องต้น) และอธิบายความหมายของแต่ละคอลัมน์ในไฟล์นี้เป็นภาษาไทย\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259f9a9b"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'Stations in the model_HII.csv' file was already loaded into the 'stations' DataFrame in a previous step. Now, I need to inspect this DataFrame by displaying its head, information, and descriptive statistics as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc3504dd",
        "outputId": "2cfeca70-b3b1-4ca1-b0b0-5e79d46b258e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(\"5 แถวแรกของ DataFrame stations:\\n\")\n",
        "print(stations.head())\n",
        "\n",
        "# Display summary information about the DataFrame\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame stations:\\n\")\n",
        "stations.info()\n",
        "\n",
        "# Display descriptive statistics of numerical columns\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame stations:\\n\")\n",
        "print(stations.describe())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 แถวแรกของ DataFrame stations:\n",
            "\n",
            "   Node        Lat        Long\n",
            "0  ACRU  15.788054  104.642235\n",
            "1  BBHN   9.536913   98.579490\n",
            "2  BBUA  17.372976  103.984436\n",
            "3  BCNG  17.386840  103.290870\n",
            "4  BDAR  14.587073  102.495190\n",
            "\n",
            "ข้อมูลสรุปของ DataFrame stations:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 385 entries, 0 to 384\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Node    385 non-null    object \n",
            " 1   Lat     385 non-null    float64\n",
            " 2   Long    385 non-null    float64\n",
            "dtypes: float64(2), object(1)\n",
            "memory usage: 9.2+ KB\n",
            "\n",
            "ค่าสถิติเชิงพรรณนาของ DataFrame stations:\n",
            "\n",
            "              Lat        Long\n",
            "count  385.000000  385.000000\n",
            "mean    14.920257  100.828891\n",
            "std      3.551041    1.556382\n",
            "min      5.939266   97.935140\n",
            "25%     13.774838   99.796160\n",
            "50%     15.632450  100.333910\n",
            "75%     17.398815  101.774750\n",
            "max     20.240082  105.501720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6351d169"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `stations`:\n",
        "\n",
        "- **Node**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝน (Station ID) ใช้ระบุสถานีแต่ละแห่งที่ไม่ซ้ำกัน\n",
        "- **Lat**: ละติจูดของสถานีวัดปริมาณน้ำฝน (Latitude) แสดงพิกัดทางภูมิศาสตร์ตามแนวนอน\n",
        "- **Long**: ลองจิจูดของสถานีวัดปริมาณน้ำฝน (Longitude) แสดงพิกัดทางภูมิศาสตร์ตามแนวตั้ง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd6f6d85"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `stations`:\n",
        "\n",
        "- **Node**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝน (Station ID) ใช้ระบุสถานีแต่ละแห่งที่ไม่ซ้ำกัน\n",
        "- **Lat**: ละติจูดของสถานีวัดปริมาณน้ำฝน (Latitude) แสดงพิกัดทางภูมิศาสตร์ตามแนวนอน\n",
        "- **Long**: ลองจิจูดของสถานีวัดปริมาณน้ำฝน (Longitude) แสดงพิกัดทางภูมิศาสตร์ตามแนวตั้ง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28aba734"
      },
      "source": [
        "## Load and Inspect Historical Daily Rainfall Data\n",
        "\n",
        "### Subtask:\n",
        "อ่านไฟล์ 'hii_daily_rain.csv' ซึ่งเป็นข้อมูลปริมาณน้ำฝนรายวันย้อนหลัง (สำหรับคำนวณค่าสถิติ) เข้ามาเก็บใน DataFrame พร้อมทั้งแปลงคอลัมน์วันที่ให้เป็นรูปแบบ Datetime จากนั้นแสดงผลข้อมูลเบื้องต้น (เช่น 5 แถวแรก, ชนิดข้อมูล, ค่าสถิติเบื้องต้น) และอธิบายความหมายของแต่ละคอลัมน์ในไฟล์นี้เป็นภาษาไทย\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11f78651"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to load the 'hii_daily_rain.csv' file into a DataFrame, convert its date column to datetime objects, and then display its head, info, and descriptive statistics as requested. After that, I will provide a markdown explanation for each column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "e835d9de",
        "outputId": "7a7331f2-6e52-4631-b043-186fbdd153da"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/hii_daily_rain.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3572932038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_daily_rainfall_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/hii_daily_rain.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddd4e586"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed with a `FileNotFoundError` because the path to the historical daily rainfall file was incorrect. Based on the initial file list, the correct filename for the historical data is `historical_daily_rainfall.csv`, and it is likely located directly in the `base_path` rather than in the `Input - (Operation Daily rain from API_ 2025)` folder. Therefore, I will redefine the `historical_daily_rainfall_file` variable before attempting to read the CSV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "f0ce5989",
        "outputId": "2969d063-c583-4c29-d538-aabb23523884"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path if not already defined (ensuring it's available in this cell's scope)\n",
        "# As per kernel state, base_path is already defined: '/content/drive/MyDrive/Data mining/midterm/'\n",
        "# However, to ensure the code is self-contained if run independently, or if kernel state changes,\n",
        "# I'll include its definition here for clarity if it was not explicitly requested to be omitted.\n",
        "# For this fix, I will assume base_path is available from previous execution.\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file based on problem description (cell 9JQPxFA1Xa_-)\n",
        "# which specifies 'historical_daily_rainfall.csv' as the historical file.\n",
        "# Assuming it's located directly under the base_path.\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/' # Ensuring base_path is available\n",
        "historical_daily_rainfall_file = os.path.join(base_path, 'historical_daily_rainfall.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'historical_daily_rainfall.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/historical_daily_rainfall.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3381678503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 1. โหลดไฟล์ 'historical_daily_rainfall.csv' เข้ามาใน DataFrame ชื่อ historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_daily_rainfall_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/historical_daily_rainfall.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b929d51"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts failed because the path to the historical daily rainfall file was incorrect. I will correct the `historical_daily_rainfall_file` variable to point to `historical_daily_rainfall.csv` inside the `input_folder_path`, which aligns with the initial file list and the overall data organization. I will also update the comment to reflect the correct filename.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8571ccc0",
        "outputId": "71389119-0f94-4155-9ec3-684fd8d05f32"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path if not already defined (ensuring it's available in this cell's scope)\n",
        "# As per kernel state, base_path is already defined: '/content/drive/MyDrive/Data mining/midterm/'\n",
        "# However, to ensure the code is self-contained if run independently, or if kernel state changes,\n",
        "# I'll include its definition here for clarity if it was not explicitly requested to be omitted.\n",
        "# For this fix, I will assume base_path is available from previous execution.\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file based on problem description (cell 9JQPxFA1Xa_-)\n",
        "# which specifies 'historical_daily_rainfall.csv' as the historical file, and assuming it's in the input folder.\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/' # Ensuring base_path is available\n",
        "input_folder_path = os.path.join(base_path, 'Input - (Operation Daily rain from API_ 2025)')\n",
        "historical_daily_rainfall_file = os.path.join(input_folder_path, 'historical_daily_rainfall.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'historical_daily_rainfall.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/historical_daily_rainfall.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-546351677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 1. โหลดไฟล์ 'historical_daily_rainfall.csv' เข้ามาใน DataFrame ชื่อ historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_daily_rainfall_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/historical_daily_rainfall.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820979a5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load the historical daily rainfall data failed due to `FileNotFoundError`. There has been conflicting information regarding the exact filename (`historical_daily_rainfall.csv` vs `hii_daily_rain.csv`) and its location. Based on the task description in cell `ce7b48b6` which explicitly mentions `hii_daily_rain.csv` for imputation, I will assume `hii_daily_rain.csv` is the correct filename for the historical data. Given that previous attempts failed to find it within the `input_folder_path` or as `historical_daily_rainfall.csv` in `base_path`, I will now place `hii_daily_rain.csv` directly in the `base_path`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "e58ea302",
        "outputId": "1f7e98d2-7ba5-46bd-b991-3d21a4366371"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path if not already defined (ensuring it's available in this cell's scope)\n",
        "# As per kernel state, base_path is already defined: '/content/drive/MyDrive/Data mining/midterm/'\n",
        "# However, to ensure the code is self-contained if run independently, or if kernel state changes,\n",
        "# I'll include its definition here for clarity if it was not explicitly requested to be omitted.\n",
        "# For this fix, I will assume base_path is available from previous execution.\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file based on problem description (cell ce7b48b6)\n",
        "# which specifies 'hii_daily_rain.csv' as the historical file, and assuming it's located directly under the base_path.\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/' # Ensuring base_path is available\n",
        "historical_daily_rainfall_file = os.path.join(base_path, 'hii_daily_rain.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/hii_daily_rain.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3913215211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_daily_rainfall_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/hii_daily_rain.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3562a7b0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load the historical daily rainfall data consistently resulted in a `FileNotFoundError`. The subtask specifically instructs to load 'hii_daily_rain.csv'. While the `Files` section mentions 'historical_daily_rainfall.csv', the `Task` description explicitly references 'hii_daily_rain.csv' for imputation. Therefore, I will prioritize the file name from the subtask and set the path to 'hii_daily_rain.csv' within the `input_folder_path`, as this folder is designated for input data. This path was attempted earlier and failed, suggesting the file might be genuinely missing from the accessible Google Drive location, but this remains the most logical programmatic attempt based on the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3325da21",
        "outputId": "4bebf09d-5db4-40bb-83e1-ceda9e3e5a01"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path and input_folder_path to ensure they are consistent with previous definitions\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/'\n",
        "input_folder_path = os.path.join(base_path, 'Input - (Operation Daily rain from API_ 2025)')\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file to 'hii_daily_rain.csv'\n",
        "# and placing it in the input_folder_path as it's an input for calculations.\n",
        "historical_daily_rainfall_file = os.path.join(input_folder_path, 'hii_daily_rain.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/hii_daily_rain.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1825362966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_daily_rainfall_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/hii_daily_rain.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ba87eb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load the historical daily rainfall data consistently resulted in a `FileNotFoundError`. To resolve the ambiguity regarding the correct filename (`historical_daily_rainfall.csv` vs `hii_daily_rain.csv`) and its precise location, I will list the contents of both the `base_path` and the `input_folder_path`. This will help identify the actual filename and its directory, allowing for correct file loading in subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68796c5f",
        "outputId": "2520312c-56b1-4f5d-8346-3e0b1842a4a7"
      },
      "source": [
        "import os\n",
        "\n",
        "# List contents of the base_path\n",
        "print(f\"\\nContents of {base_path}:\")\n",
        "for item in os.listdir(base_path):\n",
        "    print(f\"- {item}\")\n",
        "\n",
        "# List contents of the input_folder_path\n",
        "print(f\"\\nContents of {input_folder_path}:\")\n",
        "for item in os.listdir(input_folder_path):\n",
        "    print(f\"- {item}\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contents of /content/drive/MyDrive/Data mining/midterm/:\n",
            "- Input - (Operation Daily rain from API_ 2025)\n",
            "- Daily rain - (Statistics 2012-2024)\n",
            "- Output - (Clean monthly rain - 2025)\n",
            "- .ipynb_checkpoints\n",
            "\n",
            "Contents of /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025):\n",
            "- 2025-12\n",
            "- Stations in the model_HII.csv\n",
            "- 2025-08\n",
            "- 2025-10\n",
            "- 2025-07\n",
            "- 2025-09\n",
            "- 2025-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ce9559"
      },
      "source": [
        "**Reasoning**:\n",
        "The `os.listdir()` output from the previous step revealed a folder named 'Daily rain - (Statistics 2012-2024)' in the `base_path`. Given that the initial problem description (cell 9JQPxFA1Xa_-) specified 'historical_daily_rainfall.csv' as the historical rainfall data, it is highly probable that this file is located inside this new folder. I will update the `historical_daily_rainfall_file` path accordingly to attempt loading the file from this more logical location and then proceed with the rest of the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "f837ab97",
        "outputId": "6ea07abd-b845-4793-d2c0-692706371c76"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path if not already defined (ensuring it's available in this cell's scope)\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/'\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file\n",
        "# Based on the os.listdir output, 'Daily rain - (Statistics 2012-2024)' seems like the correct folder.\n",
        "# And the file name is 'historical_daily_rainfall.csv' as per problem description.\n",
        "historical_data_folder = os.path.join(base_path, 'Daily rain - (Statistics 2012-2024)')\n",
        "historical_daily_rainfall_file = os.path.join(historical_data_folder, 'historical_daily_rainfall.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'historical_daily_rainfall.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Daily rain - (Statistics 2012-2024)/historical_daily_rainfall.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3987134026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 1. โหลดไฟล์ 'historical_daily_rainfall.csv' เข้ามาใน DataFrame ชื่อ historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_daily_rainfall_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data mining/midterm/Daily rain - (Statistics 2012-2024)/historical_daily_rainfall.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c87df8d0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load the historical daily rainfall data continuously failed, even after inspecting the parent directories. To definitively locate the correct historical data file, I will list the contents of the 'Daily rain - (Statistics 2012-2024)' folder, as this is the most probable location based on its name and previous directory listings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86a4fb6a",
        "outputId": "adcf8d6b-efd1-43dd-c285-893a244e9a52"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the folder path for historical data\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/'\n",
        "historical_data_folder = os.path.join(base_path, 'Daily rain - (Statistics 2012-2024)')\n",
        "\n",
        "# List contents of the historical data folder\n",
        "print(f\"\\nContents of {historical_data_folder}:\")\n",
        "if os.path.exists(historical_data_folder):\n",
        "    for item in os.listdir(historical_data_folder):\n",
        "        print(f\"- {item}\")\n",
        "else:\n",
        "    print(f\"Folder not found: {historical_data_folder}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contents of /content/drive/MyDrive/Data mining/midterm/Daily rain - (Statistics 2012-2024):\n",
            "- hii_daily_rain.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adbeaf3e"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the `os.listdir()` output from the previous step, the historical daily rainfall file is definitively named 'hii_daily_rain.csv' and is located within the 'Daily rain - (Statistics 2012-2024)' folder. I will now use this corrected path to load the data, convert the date column, and display its head, info, and descriptive statistics as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "501f9792",
        "outputId": "39d53210-a303-4ace-b00e-a063c2d7176e"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path and historical_data_folder to ensure consistency\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/'\n",
        "historical_data_folder = os.path.join(base_path, 'Daily rain - (Statistics 2012-2024)')\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file based on the os.listdir output\n",
        "historical_daily_rainfall_file = os.path.join(historical_data_folder, 'hii_daily_rain.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['date'] = pd.to_datetime(historical_df['date'], format='%Y-%m-%d')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-999978911.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistorical_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 3. แสดง 5 แถวแรกของ DataFrame historical_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'date'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2c669c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `KeyError: 'date'` because the column containing date information in `historical_df` is named `measure_datetime`, not `date`. I will correct the column name and the datetime format string to properly parse the `measure_datetime` column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ee8add",
        "outputId": "3f7808e6-0336-48b1-b009-5944d9c91ed5"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Redefine base_path and historical_data_folder to ensure consistency\n",
        "base_path = '/content/drive/MyDrive/Data mining/midterm/'\n",
        "historical_data_folder = os.path.join(base_path, 'Daily rain - (Statistics 2012-2024)')\n",
        "\n",
        "# Correcting the path for the historical daily rainfall file based on the os.listdir output\n",
        "historical_daily_rainfall_file = os.path.join(historical_data_folder, 'hii_daily_rain.csv')\n",
        "\n",
        "# 1. โหลดไฟล์ 'hii_daily_rain.csv' เข้ามาใน DataFrame ชื่อ historical_df\n",
        "historical_df = pd.read_csv(historical_daily_rainfall_file)\n",
        "\n",
        "# 2. แปลงคอลัมน์ที่เก็บข้อมูลวันที่ให้เป็นรูปแบบ Datetime\n",
        "historical_df['measure_datetime'] = pd.to_datetime(historical_df['measure_datetime'], format='%Y-%m-%d %H:%M:%S%z')\n",
        "\n",
        "# 3. แสดง 5 แถวแรกของ DataFrame historical_df\n",
        "print(\"\\n5 แถวแรกของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.head())\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame historical_df\n",
        "print(\"\\nข้อมูลสรุปของ DataFrame historical_df:\\n\")\n",
        "historical_df.info()\n",
        "\n",
        "# 5. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน DataFrame historical_df\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\\n\")\n",
        "print(historical_df.describe())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 แถวแรกของ DataFrame historical_df:\n",
            "\n",
            "  station_code          measure_datetime  data station_name   latitude  \\\n",
            "0         ABRT 2012-01-01 07:00:00+07:00   0.0   วัดเวฬุวัน  16.054785   \n",
            "1         ABRT 2012-01-03 07:00:00+07:00   0.0   วัดเวฬุวัน  16.054785   \n",
            "2         ABRT 2012-01-06 07:00:00+07:00   0.0   วัดเวฬุวัน  16.054785   \n",
            "3         ABRT 2012-01-07 07:00:00+07:00   0.0   วัดเวฬุวัน  16.054785   \n",
            "4         ABRT 2012-01-08 07:00:00+07:00   0.0   วัดเวฬุวัน  16.054785   \n",
            "\n",
            "   longitude quality_flag basin           sub_basin      tambon  \\\n",
            "0  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "1  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "2  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "3  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "4  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "\n",
            "          amphoe  province       data_type  \n",
            "0  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "1  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "2  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "3  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "4  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "\n",
            "ข้อมูลสรุปของ DataFrame historical_df:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3768463 entries, 0 to 3768462\n",
            "Data columns (total 13 columns):\n",
            " #   Column            Dtype                    \n",
            "---  ------            -----                    \n",
            " 0   station_code      object                   \n",
            " 1   measure_datetime  datetime64[ns, UTC+07:00]\n",
            " 2   data              float64                  \n",
            " 3   station_name      object                   \n",
            " 4   latitude          float64                  \n",
            " 5   longitude         float64                  \n",
            " 6   quality_flag      object                   \n",
            " 7   basin             object                   \n",
            " 8   sub_basin         object                   \n",
            " 9   tambon            object                   \n",
            " 10  amphoe            object                   \n",
            " 11  province          object                   \n",
            " 12  data_type         object                   \n",
            "dtypes: datetime64[ns, UTC+07:00](1), float64(3), object(9)\n",
            "memory usage: 373.8+ MB\n",
            "\n",
            "ค่าสถิติเชิงพรรณนาของ DataFrame historical_df:\n",
            "\n",
            "               data      latitude     longitude\n",
            "count  3.768326e+06  3.768463e+06  3.768463e+06\n",
            "mean   3.239066e+00  1.472161e+01  1.008651e+02\n",
            "std    1.022313e+01  3.619553e+00  1.639000e+00\n",
            "min    0.000000e+00  5.775882e+00  9.785223e+01\n",
            "25%    0.000000e+00  1.352140e+01  9.968776e+01\n",
            "50%    0.000000e+00  1.531081e+01  1.004387e+02\n",
            "75%    8.000000e-01  1.732073e+01  1.018452e+02\n",
            "max    2.054400e+03  2.047149e+01  1.056168e+02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f76161a"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `historical_df`:\n",
        "\n",
        "- **station_code (Node)**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝน ใช้ระบุสถานีแต่ละแห่งที่ไม่ซ้ำกัน\n",
        "- **measure_datetime (date)**: วันที่และเวลาที่ทำการวัดปริมาณน้ำฝน โดยถูกแปลงให้อยู่ในรูปแบบ Datetime\n",
        "- **data (rain_mm)**: ปริมาณน้ำฝนที่วัดได้ในแต่ละวัน มีหน่วยเป็นมิลลิเมตร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fabd1a82"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `historical_df`:\n",
        "\n",
        "- **station_code (Node)**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝน ใช้ระบุสถานีแต่ละแห่งที่ไม่ซ้ำกัน\n",
        "- **measure_datetime (date)**: วันที่และเวลาที่ทำการวัดปริมาณน้ำฝน โดยถูกแปลงให้อยู่ในรูปแบบ Datetime\n",
        "- **data (rain_mm)**: ปริมาณน้ำฝนที่วัดได้ในแต่ละวัน มีหน่วยเป็นมิลลิเมตร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2254b503"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `historical_df`:\n",
        "\n",
        "- **station_code (Node)**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝน ใช้ระบุสถานีแต่ละแห่งที่ไม่ซ้ำกัน\n",
        "- **measure_datetime (date)**: วันที่และเวลาที่ทำการวัดปริมาณน้ำฝน โดยถูกแปลงให้อยู่ในรูปแบบ Datetime\n",
        "- **data (rain_mm)**: ปริมาณน้ำฝนที่วัดได้ในแต่ละวัน มีหน่วยเป็นมิลลิเมตร\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "463efb2c"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `historical_df`:\n",
        "\n",
        "- **station_code (Node)**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝน ใช้ระบุสถานีแต่ละแห่งที่ไม่ซ้ำกัน\n",
        "- **measure_datetime (date)**: วันที่และเวลาที่ทำการวัดปริมาณน้ำฝน โดยถูกแปลงให้อยู่ในรูปแบบ Datetime\n",
        "- **data (rain_mm)**: ปริมาณน้ำฝนที่วัดได้ในแต่ละวัน มีหน่วยเป็นมิลลิเมตร\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb30fd0"
      },
      "source": [
        "## Load and Inspect Operational Daily Rainfall Data\n",
        "\n",
        "### Subtask:\n",
        "อ่านไฟล์ทั้งหมดในโฟลเดอร์ 'Input-(Operation Daily rain from API_2025)' ที่ไม่ได้เป็นไฟล์เฉพาะ (เช่น historical, stations, output) และถือว่าเป็นไฟล์ข้อมูลปริมาณน้ำฝนรายวันที่ต้องการประมวลผล (แม้ว่าจะไม่มีนามสกุล .csv ก็ตาม) มารวมกันใน DataFrame เดียว พร้อมทั้งแปลงคอลัมน์วันที่ให้เป็นรูปแบบ Datetime และคอลัมน์ปริมาณน้ำฝนให้เป็นตัวเลข จากนั้นแสดงผลข้อมูลเบื้องต้น (เช่น 5 แถวแรก, ชนิดข้อมูล, ค่าสถิติเบื้องต้น) และอธิบายความหมายของแต่ละคอลัมน์ในไฟล์นี้เป็นภาษาไทย\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88bc4b8f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully loaded the historical daily rainfall data. Now, as per the subtask, I need to load all operational daily rainfall data files from the 'Input-(Operation Daily rain from API_2025)' folder, filter out specific files, and then concatenate them into a single DataFrame for inspection. I will also handle datetime and numeric conversions for the relevant columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d80b82ed",
        "outputId": "7f8a4af7-0148-4cc7-bd47-47662f3cfee9"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1. สร้างลิสต์เปล่าชื่อ operational_data_frames\n",
        "operational_data_frames = []\n",
        "\n",
        "# 2. กำหนดลิสต์ของชื่อไฟล์ที่ไม่ต้องการประมวลผล\n",
        "# The variable `input_folder_path` and `stations_model_file` are already defined.\n",
        "# For `historical_daily_rainfall_file`, in this context, the request implies\n",
        "# excluding it if found within the operational input folder. The actual historical_daily_rainfall_file\n",
        "# was found elsewhere, but for robustness, we'll keep its potential filename in the excluded list.\n",
        "excluded_filenames = [\n",
        "    os.path.basename(stations_model_file), # e.g., 'Stations in the model_HII.csv'\n",
        "    'hii_daily_rain.csv', # As per general task description and for imputation\n",
        "    'historical_daily_rainfall.csv' # For completeness if it were mistakenly placed here\n",
        "]\n",
        "\n",
        "# To verify the contents of the folder for dynamic exclusion if necessary\n",
        "# print(f\"\\nContents of {input_folder_path}:\")\n",
        "# for item in os.listdir(input_folder_path):\n",
        "#     print(f\"- {item}\")\n",
        "\n",
        "# 3. วนลูปอ่านไฟล์ทั้งหมดใน input_folder_path\n",
        "print(f\"Reading files from: {input_folder_path}\")\n",
        "for filename in os.listdir(input_folder_path):\n",
        "    # Check if the file is an actual file and not a directory\n",
        "    file_path = os.path.join(input_folder_path, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        # Check if the filename is in the excluded list\n",
        "        if filename in excluded_filenames:\n",
        "            print(f\"Skipping excluded file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Check if the file is a CSV. The instruction says \"แม้ว่าจะไม่มีนามสกุล .csv ก็ตาม\" meaning it might not have .csv extension.\n",
        "        # However, to avoid errors with non-csv files, we will try to read it as CSV and handle errors.\n",
        "        # For now, let's assume valid operational data files are intended to be CSVs that might sometimes lack the extension,\n",
        "        # or the problem implies they are monthly folders containing daily CSVs.\n",
        "        # Given the previous `os.listdir(input_folder_path)` output had `2025-12`, `2025-08` etc., these are likely folders.\n",
        "        # The subtask says \"อ่านไฟล์ทั้งหมดในโฟลเดอร์\", implying individual files are to be read.\n",
        "        # Let's adjust this to read files within these monthly folders if they are CSVs, or if they directly contain data.\n",
        "        # For this step, I will assume the operational files are directly within 'input_folder_path' and are CSVs by nature if they are not in the excluded list.\n",
        "\n",
        "        # The previous os.listdir output showed folders like '2025-12', '2025-08', etc.\n",
        "        # This suggests the operational daily rainfall data is likely *inside* these monthly folders.\n",
        "        # The instruction \"อ่านไฟล์ทั้งหมดในโฟลเดอร์ 'Input-(Operation Daily rain from API_2025)'\" could mean files directly in it,\n",
        "        # or recursively within subfolders. Given the output, it's likely subfolders.\n",
        "        # However, the task implies these are daily rainfall files, so let's check for `.csv` extension, and if not present, assume it is not an operational file.\n",
        "\n",
        "        if not filename.endswith('.csv'):\n",
        "            print(f\"Skipping non-CSV file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            temp_df = pd.read_csv(file_path)\n",
        "            print(f\"Successfully loaded {filename}\")\n",
        "\n",
        "            # Convert 'measure_datetime' to datetime\n",
        "            # Check if 'measure_datetime' column exists before conversion\n",
        "            if 'measure_datetime' in temp_df.columns:\n",
        "                temp_df['measure_datetime'] = pd.to_datetime(temp_df['measure_datetime'], format='%Y-%m-%d %H:%M:%S%z')\n",
        "            else:\n",
        "                print(f\"Warning: 'measure_datetime' column not found in {filename}. Skipping datetime conversion.\")\n",
        "\n",
        "            # Convert 'data' column to numeric\n",
        "            if 'data' in temp_df.columns:\n",
        "                temp_df['data'] = pd.to_numeric(temp_df['data'], errors='coerce')\n",
        "            else:\n",
        "                print(f\"Warning: 'data' column not found in {filename}. Skipping numeric conversion.\")\n",
        "\n",
        "            operational_data_frames.append(temp_df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filename}: {e}\")\n",
        "    else:\n",
        "        # It's a directory, not a file, so we skip it for this direct file loading step\n",
        "        print(f\"Skipping directory: {filename}\")\n",
        "\n",
        "# 4. รวม DataFrame ทั้งหมดในลิสต์ operational_data_frames เข้าด้วยกัน\n",
        "if operational_data_frames:\n",
        "    operational_daily_rainfall_df = pd.concat(operational_data_frames, ignore_index=True)\n",
        "    print(\"\\nAll operational daily rainfall data concatenated.\")\n",
        "\n",
        "    # 5. แสดง 5 แถวแรกของ operational_daily_rainfall_df\n",
        "    print(\"\\n5 แถวแรกของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    print(operational_daily_rainfall_df.head())\n",
        "\n",
        "    # 6. แสดงข้อมูลสรุปของ operational_daily_rainfall_df\n",
        "    print(\"\\nข้อมูลสรุปของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    operational_daily_rainfall_df.info()\n",
        "\n",
        "    # 7. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน operational_daily_rainfall_df\n",
        "    print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    print(operational_daily_rainfall_df.describe())\n",
        "else:\n",
        "    print(\"No operational daily rainfall data files were found or processed.\")\n",
        "    # Initialize an empty DataFrame if no files were processed\n",
        "    operational_daily_rainfall_df = pd.DataFrame()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files from: /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)\n",
            "Skipping directory: 2025-12\n",
            "Skipping excluded file: Stations in the model_HII.csv\n",
            "Skipping directory: 2025-08\n",
            "Skipping directory: 2025-10\n",
            "Skipping directory: 2025-07\n",
            "Skipping directory: 2025-09\n",
            "Skipping directory: 2025-11\n",
            "No operational daily rainfall data files were found or processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ed08370"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed to load operational daily rainfall data because the files are located within monthly subfolders inside the `input_folder_path`, not directly in it. I need to modify the code to recursively traverse these subfolders, identify potential data files, and then attempt to read them into DataFrames while applying the specified transformations and exclusions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ee37c3",
        "outputId": "8a791517-79be-4470-a866-d47128424f5e"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1. สร้างลิสต์เปล่าชื่อ operational_data_frames\n",
        "operational_data_frames = []\n",
        "\n",
        "# 2. กำหนดลิสต์ของชื่อไฟล์ที่ไม่ต้องการประมวลผล\n",
        "# The variable `input_folder_path` and `stations_model_file` are already defined.\n",
        "# For `historical_daily_rainfall_file`, in this context, the request implies\n",
        "# excluding it if found within the operational input folder.\n",
        "excluded_filenames = [\n",
        "    os.path.basename(stations_model_file), # e.g., 'Stations in the model_HII.csv'\n",
        "    'hii_daily_rain.csv', # As per general task description and for imputation\n",
        "    'historical_daily_rainfall.csv' # For completeness if it were mistakenly placed here\n",
        "]\n",
        "\n",
        "# 3. วนลูปอ่านไฟล์ทั้งหมดใน input_folder_path และ sub-folders\n",
        "print(f\"Searching for operational daily rainfall files in: {input_folder_path}\")\n",
        "for root, dirs, files in os.walk(input_folder_path):\n",
        "    for filename in files:\n",
        "        # Construct the full path to the file\n",
        "        file_path = os.path.join(root, filename)\n",
        "\n",
        "        # Check if the filename is in the excluded list (base filename only)\n",
        "        if filename in excluded_filenames:\n",
        "            print(f\"Skipping excluded file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Check if the file has a .csv extension or if it's meant to be read as a CSV\n",
        "        # The instruction says \"แม้ว่าจะไม่มีนามสกุล .csv ก็ตาม\" meaning it might not have .csv extension.\n",
        "        # To handle this, we'll try to read all non-excluded files as CSVs and catch errors.\n",
        "\n",
        "        try:\n",
        "            temp_df = pd.read_csv(file_path)\n",
        "            print(f\"Successfully loaded {filename} from {root}\")\n",
        "\n",
        "            # Convert 'measure_datetime' to datetime\n",
        "            # Check if 'measure_datetime' column exists before conversion\n",
        "            if 'measure_datetime' in temp_df.columns:\n",
        "                temp_df['measure_datetime'] = pd.to_datetime(temp_df['measure_datetime'], format='%Y-%m-%d %H:%M:%S%z', errors='coerce')\n",
        "                # Drop rows where datetime conversion failed if any\n",
        "                temp_df.dropna(subset=['measure_datetime'], inplace=True)\n",
        "            else:\n",
        "                print(f\"Warning: 'measure_datetime' column not found in {filename}. Skipping datetime conversion.\")\n",
        "\n",
        "            # Convert 'data' column to numeric\n",
        "            if 'data' in temp_df.columns:\n",
        "                temp_df['data'] = pd.to_numeric(temp_df['data'], errors='coerce')\n",
        "                # Drop rows where data conversion failed if any\n",
        "                temp_df.dropna(subset=['data'], inplace=True)\n",
        "            else:\n",
        "                print(f\"Warning: 'data' column not found in {filename}. Skipping numeric conversion.\")\n",
        "\n",
        "            # Only append if the DataFrame has actual data after processing\n",
        "            if not temp_df.empty:\n",
        "                operational_data_frames.append(temp_df)\n",
        "            else:\n",
        "                print(f\"Warning: {filename} became empty after processing. Skipping.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading or processing {filename} from {root}: {e}\")\n",
        "\n",
        "# 4. รวม DataFrame ทั้งหมดในลิสต์ operational_data_frames เข้าด้วยกัน\n",
        "if operational_data_frames:\n",
        "    operational_daily_rainfall_df = pd.concat(operational_data_frames, ignore_index=True)\n",
        "    print(\"\\nAll operational daily rainfall data concatenated.\")\n",
        "\n",
        "    # 5. แสดง 5 แถวแรกของ operational_daily_rainfall_df\n",
        "    print(\"\\n5 แถวแรกของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    print(operational_daily_rainfall_df.head())\n",
        "\n",
        "    # 6. แสดงข้อมูลสรุปของ operational_daily_rainfall_df\n",
        "    print(\"\\nข้อมูลสรุปของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    operational_daily_rainfall_df.info()\n",
        "\n",
        "    # 7. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน operational_daily_rainfall_df\n",
        "    print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    print(operational_daily_rainfall_df.describe())\n",
        "else:\n",
        "    print(\"No operational daily rainfall data files were found or processed.\")\n",
        "    # Initialize an empty DataFrame if no files were processed\n",
        "    operational_daily_rainfall_df = pd.DataFrame()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for operational daily rainfall files in: /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)\n",
            "Skipping excluded file: Stations in the model_HII.csv\n",
            "Successfully loaded HII_daily_rainfall_2025-12-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-02.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-02.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-03.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-03.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-09.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-09.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-04.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-04.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-08.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-08.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-11.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-11.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-10.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-10.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-06.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-06.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-01.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-01.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-05.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-05.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-07.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-07.csv became empty after processing. Skipping.\n",
            "Error reading or processing station_missing_data_report_December 2025.jpg from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
            "Successfully loaded HII_daily_rainfall_2025-12-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-29.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-29.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-14.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-14.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-27.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-27.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-28.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-28.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-15.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-15.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-24.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-24.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-16.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-16.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-18.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-18.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-12.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-12.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-30.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-30.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-17.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-17.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-19.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-19.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-20.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-20.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-22.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-22.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-23.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-23.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-31.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-31.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-26.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-26.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-25.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-25.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-21.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-21.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-12-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-12-13.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-12-13.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-02.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-02.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-04.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-04.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-03.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-03.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-01.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-01.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-27.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-27.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-07.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-07.csv became empty after processing. Skipping.\n",
            "Successfully loaded rain_hii_20250831.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in rain_hii_20250831.csv. Skipping numeric conversion.\n",
            "Warning: rain_hii_20250831.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-19.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-19.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-13.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-13.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-05.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-05.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-30.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-30.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-11.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-11.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-31.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-31.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-06.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-06.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-14.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-14.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-16.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-16.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-25.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-25.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-24.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-24.csv became empty after processing. Skipping.\n",
            "Error reading or processing station_missing_data_report_August 2025.jpg from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
            "Successfully loaded HII_daily_rainfall_2025-08-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-15.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-15.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-08.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-08.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-21.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-21.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-18.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-18.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-23.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-23.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-22.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-22.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-12.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-12.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-17.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-17.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-28.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-28.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-26.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-26.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-20.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-20.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-10.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-10.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-09.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-09.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-08-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-08-29.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-08-29.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-07.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-07.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-14.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-14.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-22.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-22.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-01.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-01.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-10.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-10.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-12.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-12.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-16.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-16.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-13.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-13.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-04.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-04.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-06.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-06.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-20.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-20.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-23.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-23.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-03.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-03.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-17.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-17.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-02.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-02.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-15.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-15.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-09.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-09.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-21.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-21.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-19.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-19.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-05.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-05.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-11.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-11.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-08.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-08.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-18.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-18.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-27.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-27.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-31.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-31.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-26.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-26.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-25.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-25.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-30.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-30.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-28.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-28.csv became empty after processing. Skipping.\n",
            "Error reading or processing station_missing_data_report_October 2025.jpg from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
            "Successfully loaded HII_daily_rainfall_2025-10-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-29.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-29.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-10-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-10-24.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-10-24.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-09.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-09.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-10.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-10.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-02.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-02.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-01.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-01.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-06.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-06.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-11.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-11.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-03.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-03.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-08.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-08.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-07.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-07.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-04.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-04.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-05.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-05.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-14.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-14.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-25.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-25.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-24.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-24.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-13.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-13.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-20.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-20.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-27.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-27.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-18.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-18.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-26.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-26.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-17.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-17.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-30.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-30.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-29.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-29.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-31.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-31.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-15.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-15.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-12.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-12.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-22.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-22.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-19.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-19.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-16.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-16.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-21.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-21.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-23.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-23.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-07-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-07-28.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-07-28.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-23.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-23.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-28.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-28.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-12.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-12.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-26.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-26.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-02.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-02.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-24.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-24.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-15.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-15.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-03.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-03.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-20.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-20.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-07.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-07.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-13.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-13.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-17.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-17.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-04.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-04.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-29.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-29.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-14.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-14.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-27.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-27.csv became empty after processing. Skipping.\n",
            "Successfully loaded rain_hii_20250901.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in rain_hii_20250901.csv. Skipping numeric conversion.\n",
            "Warning: rain_hii_20250901.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-06.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-06.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-21.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-21.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-01.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-01.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-05.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-05.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-18.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-18.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-09.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-09.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-22.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-22.csv became empty after processing. Skipping.\n",
            "Successfully loaded rain_hii_20250902.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in rain_hii_20250902.csv. Skipping numeric conversion.\n",
            "Warning: rain_hii_20250902.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-25.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-25.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-11.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-11.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-19.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-19.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-30.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-30.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-08.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-08.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-16.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-16.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-09-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-09-10.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-09-10.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-07.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-07.csv became empty after processing. Skipping.\n",
            "Error reading or processing station_missing_data_report_November 2025.jpg from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
            "Successfully loaded HII_daily_rainfall_2025-11-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-24.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-24.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-27.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-27.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-15.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-15.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-21.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-21.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-09.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-09.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-01.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-01.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-25.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-25.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-05.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-05.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-20.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-20.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-26.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-26.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-02.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-02.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-22.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-22.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-19.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-19.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-28.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-28.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-06.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-06.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-16.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-16.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-13.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-13.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-30.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-30.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-10.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-10.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-17.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-17.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-03.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-03.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-11.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-11.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-14.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-14.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-29.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-29.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-08.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-08.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-18.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-18.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-23.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-23.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-04.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-04.csv became empty after processing. Skipping.\n",
            "Successfully loaded HII_daily_rainfall_2025-11-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11\n",
            "Warning: 'data' column not found in HII_daily_rainfall_2025-11-12.csv. Skipping numeric conversion.\n",
            "Warning: HII_daily_rainfall_2025-11-12.csv became empty after processing. Skipping.\n",
            "No operational daily rainfall data files were found or processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e46b13a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed to load operational daily rainfall data because the correct column names for datetime and rainfall amount (`rainfall_datetime` and `rainfall_value`) were not being used, leading to DataFrames becoming empty after dropping missing values. I will modify the code to prioritize these column names for conversion and ensure the data is loaded correctly, then proceed to display the head, info, and descriptive statistics as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83b7c0c",
        "outputId": "63d98bc1-6227-46cf-d524-6ab8bd2782ea"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1. สร้างลิสต์เปล่าชื่อ operational_data_frames\n",
        "operational_data_frames = []\n",
        "\n",
        "# 2. กำหนดลิสต์ของชื่อไฟล์ที่ไม่ต้องการประมวลผล\n",
        "# The variable `input_folder_path` and `stations_model_file` are already defined.\n",
        "# For `historical_daily_rainfall_file`, in this context, the request implies\n",
        "# excluding it if found within the operational input folder.\n",
        "excluded_filenames = [\n",
        "    os.path.basename(stations_model_file), # e.g., 'Stations in the model_HII.csv'\n",
        "    'hii_daily_rain.csv', # As per general task description and for imputation\n",
        "    'historical_daily_rainfall.csv' # For completeness if it were mistakenly placed here\n",
        "]\n",
        "\n",
        "# 3. วนลูปอ่านไฟล์ทั้งหมดใน input_folder_path และ sub-folders\n",
        "print(f\"Searching for operational daily rainfall files in: {input_folder_path}\")\n",
        "for root, dirs, files in os.walk(input_folder_path):\n",
        "    for filename in files:\n",
        "        # Construct the full path to the file\n",
        "        file_path = os.path.join(root, filename)\n",
        "\n",
        "        # Check if the filename is in the excluded list (base filename only)\n",
        "        if filename in excluded_filenames:\n",
        "            print(f\"Skipping excluded file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Check if the file has a .csv extension. Only process CSVs here.\n",
        "        if not filename.lower().endswith('.csv'):\n",
        "            print(f\"Skipping non-CSV file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            temp_df = pd.read_csv(file_path)\n",
        "            # print(f\"Successfully loaded {filename} from {root}\")\n",
        "\n",
        "            # Determine the correct datetime column\n",
        "            datetime_col = None\n",
        "            if 'rainfall_datetime' in temp_df.columns:\n",
        "                datetime_col = 'rainfall_datetime'\n",
        "            elif 'measure_datetime' in temp_df.columns:\n",
        "                datetime_col = 'measure_datetime'\n",
        "\n",
        "            if datetime_col:\n",
        "                temp_df[datetime_col] = pd.to_datetime(temp_df[datetime_col], errors='coerce')\n",
        "                # Rename to a standard 'date' column for consistency if needed later, but for now, use original\n",
        "            else:\n",
        "                print(f\"Warning: Neither 'rainfall_datetime' nor 'measure_datetime' column found in {filename}. Skipping datetime conversion.\")\n",
        "                continue # Skip if no date column is found\n",
        "\n",
        "            # Determine the correct rainfall data column\n",
        "            data_col = None\n",
        "            if 'rainfall_value' in temp_df.columns:\n",
        "                data_col = 'rainfall_value'\n",
        "            elif 'data' in temp_df.columns:\n",
        "                data_col = 'data'\n",
        "\n",
        "            if data_col:\n",
        "                temp_df[data_col] = pd.to_numeric(temp_df[data_col], errors='coerce')\n",
        "                # Rename to a standard 'data' column for consistency if needed later, but for now, use original\n",
        "            else:\n",
        "                print(f\"Warning: Neither 'rainfall_value' nor 'data' column found in {filename}. Skipping numeric conversion.\")\n",
        "                continue # Skip if no data column is found\n",
        "\n",
        "            # Drop rows where critical columns (datetime or data) are NaN after conversion\n",
        "            if datetime_col and data_col:\n",
        "                temp_df.dropna(subset=[datetime_col, data_col], inplace=True)\n",
        "\n",
        "            # Only append if the DataFrame has actual data after processing\n",
        "            if not temp_df.empty:\n",
        "                operational_data_frames.append(temp_df)\n",
        "                print(f\"Successfully loaded and processed {filename} from {root}. Rows: {len(temp_df)}\")\n",
        "            else:\n",
        "                print(f\"Warning: {filename} became empty after processing. Skipping.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading or processing {filename} from {root}: {e}\")\n",
        "\n",
        "# 4. รวม DataFrame ทั้งหมดในลิสต์ operational_data_frames เข้าด้วยกัน\n",
        "if operational_data_frames:\n",
        "    operational_daily_rainfall_df = pd.concat(operational_data_frames, ignore_index=True)\n",
        "    print(\"\\nAll operational daily rainfall data concatenated.\")\n",
        "\n",
        "    # 5. แสดง 5 แถวแรกของ operational_daily_rainfall_df\n",
        "    print(\"\\n5 แถวแรกของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    print(operational_daily_rainfall_df.head())\n",
        "\n",
        "    # 6. แสดงข้อมูลสรุปของ operational_daily_rainfall_df\n",
        "    print(\"\\nข้อมูลสรุปของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    operational_daily_rainfall_df.info()\n",
        "\n",
        "    # 7. แสดงค่าสถิติเชิงพรรณนาสำหรับคอลัมน์ที่เป็นตัวเลขใน operational_daily_rainfall_df\n",
        "    print(\"\\nค่าสถิติเชิงพรรณนาของ DataFrame operational_daily_rainfall_df:\\n\")\n",
        "    print(operational_daily_rainfall_df.describe())\n",
        "else:\n",
        "    print(\"No operational daily rainfall data files were found or processed.\")\n",
        "    # Initialize an empty DataFrame if no files were processed\n",
        "    operational_daily_rainfall_df = pd.DataFrame()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for operational daily rainfall files in: /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)\n",
            "Skipping excluded file: Stations in the model_HII.csv\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1055\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1055\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1064\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1054\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1059\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1064\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1065\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1053\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1054\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1055\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1057\n",
            "Skipping non-CSV file: station_missing_data_report_December 2025.jpg\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1053\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1060\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1051\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1053\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1057\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1055\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1052\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1050\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1059\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1048\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1058\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1052\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1052\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1052\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1050\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1050\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1050\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1050\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1049\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-12-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-12. Rows: 1063\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 889\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 889\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 889\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 891\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 896\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 888\n",
            "Warning: Neither 'rainfall_value' nor 'data' column found in rain_hii_20250831.csv. Skipping numeric conversion.\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 886\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 888\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 884\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 891\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 1268\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 884\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 885\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 885\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 895\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 878\n",
            "Skipping non-CSV file: station_missing_data_report_August 2025.jpg\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 882\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 888\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 884\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 887\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 882\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 882\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 890\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 885\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 891\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 894\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 883\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 892\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 890\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-08-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-08. Rows: 897\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 923\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 936\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 972\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 915\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 930\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 937\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 940\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 933\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 920\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 927\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 961\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 975\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 919\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 943\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 914\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 940\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 926\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 962\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 955\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 925\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 929\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 926\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 951\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 1001\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 1001\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 995\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 989\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 994\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 997\n",
            "Skipping non-CSV file: station_missing_data_report_October 2025.jpg\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 996\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-10-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-10. Rows: 982\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 900\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 904\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 909\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 905\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 905\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 905\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 901\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 904\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 906\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 895\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 898\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 896\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 901\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 893\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 898\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 896\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 888\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 894\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-31.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 893\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 895\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 901\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 902\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 897\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 901\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 897\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-07-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-07. Rows: 896\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 907\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 906\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 901\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 1270\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 906\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 902\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 899\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 904\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 898\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 901\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 902\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 894\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 910\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 904\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 907\n",
            "Warning: Neither 'rainfall_value' nor 'data' column found in rain_hii_20250901.csv. Skipping numeric conversion.\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 897\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 908\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 1276\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 898\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 900\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 902\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 906\n",
            "Warning: Neither 'rainfall_value' nor 'data' column found in rain_hii_20250902.csv. Skipping numeric conversion.\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 904\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 897\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 897\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 913\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 895\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 893\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-09-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-09. Rows: 900\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-07.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1029\n",
            "Skipping non-CSV file: station_missing_data_report_November 2025.jpg\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-24.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1045\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-27.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1039\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-15.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1047\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-21.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1047\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-09.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1031\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-01.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1009\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-25.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1037\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-05.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1023\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-20.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1048\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-26.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1045\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-02.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1005\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-22.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1046\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-19.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1049\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-28.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1030\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-06.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1029\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-16.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1043\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-13.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1042\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-30.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1050\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-10.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1034\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-17.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1042\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-03.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1012\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-11.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1035\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-14.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1048\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-29.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1041\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-08.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1034\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-18.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1042\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-23.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1048\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-04.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1018\n",
            "Successfully loaded and processed HII_daily_rainfall_2025-11-12.csv from /content/drive/MyDrive/Data mining/midterm/Input - (Operation Daily rain from API_ 2025)/2025-11. Rows: 1038\n",
            "\n",
            "All operational daily rainfall data concatenated.\n",
            "\n",
            "5 แถวแรกของ DataFrame operational_daily_rainfall_df:\n",
            "\n",
            "          rainfall_datetime     rainfall_datetime_calc  rainfall_value  \\\n",
            "0 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "1 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "2 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "3 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "4 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "\n",
            "   tele_station_id tele_station_oldcode measure_datetime  \n",
            "0        1109576.0               ONE087       2025-12-02  \n",
            "1            135.0               VLGE03       2025-12-02  \n",
            "2            134.0               TRD004       2025-12-02  \n",
            "3            114.0                 UTOG       2025-12-02  \n",
            "4            103.0               TB0004       2025-12-02  \n",
            "\n",
            "ข้อมูลสรุปของ DataFrame operational_daily_rainfall_df:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 176882 entries, 0 to 176881\n",
            "Data columns (total 6 columns):\n",
            " #   Column                  Non-Null Count   Dtype                    \n",
            "---  ------                  --------------   -----                    \n",
            " 0   rainfall_datetime       176882 non-null  datetime64[ns, UTC+07:00]\n",
            " 1   rainfall_datetime_calc  176882 non-null  object                   \n",
            " 2   rainfall_value          176882 non-null  float64                  \n",
            " 3   tele_station_id         173068 non-null  float64                  \n",
            " 4   tele_station_oldcode    168017 non-null  object                   \n",
            " 5   measure_datetime        176882 non-null  object                   \n",
            "dtypes: datetime64[ns, UTC+07:00](1), float64(2), object(3)\n",
            "memory usage: 8.1+ MB\n",
            "\n",
            "ค่าสถิติเชิงพรรณนาของ DataFrame operational_daily_rainfall_df:\n",
            "\n",
            "       rainfall_value  tele_station_id\n",
            "count   176882.000000     1.730680e+05\n",
            "mean         3.604778     2.808681e+05\n",
            "std         11.090095     4.959437e+05\n",
            "min          0.000000     1.000000e+00\n",
            "25%          0.000000     3.830000e+02\n",
            "50%          0.000000     6.870000e+02\n",
            "75%          1.400000     4.792300e+05\n",
            "max        574.800000     3.129891e+06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0ea473f"
      },
      "source": [
        "### คำอธิบายความหมายของแต่ละคอลัมน์ใน DataFrame `operational_daily_rainfall_df`:\n",
        "\n",
        "- **rainfall_datetime**: วันที่และเวลาที่ทำการวัดปริมาณน้ำฝน โดยเป็นข้อมูลหลักที่ถูกแปลงให้อยู่ในรูปแบบ Datetime\n",
        "- **rainfall_datetime_calc**: คอลัมน์เสริมที่อาจมาจากข้อมูลวันที่และเวลาเดิม ซึ่งมีรูปแบบการแสดงผลเป็น Object\n",
        "- **rainfall_value**: ปริมาณน้ำฝนที่วัดได้ในแต่ละวัน มีหน่วยเป็นมิลลิเมตร\n",
        "- **tele_station_id**: รหัสประจำตัวของสถานีวัดปริมาณน้ำฝนในระบบโทรมาตร (Telemetry Station ID)\n",
        "- **tele_station_oldcode**: รหัสประจำตัวเดิมของสถานีวัดปริมาณน้ำฝนในระบบโทรมาตร (Old Telemetry Station Code)\n",
        "- **measure_datetime**: คอลัมน์ที่อาจมาจากข้อมูลวันที่และเวลาเดิมในรูปแบบข้อความ (Object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03d96e6"
      },
      "source": [
        "## Filter Stations\n",
        "\n",
        "### Subtask:\n",
        "กรองข้อมูลปริมาณน้ำฝนรายวันในส่วนของข้อมูลที่ต้องการประมวลผล (Operational Daily Rainfall Data) ให้เหลือเฉพาะสถานีที่มีอยู่ในไฟล์ 'Stations in the model_HII.csv' เท่านั้น โดยจะแสดงจำนวนสถานีก่อนและหลังการกรองเพื่อยืนยันผลลัพธ์\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5e7e05"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to filter the `operational_daily_rainfall_df` to keep only the stations present in the `stations` DataFrame. First, I will extract the unique station IDs from the `stations` DataFrame. Then, I will display the number of unique stations in `operational_daily_rainfall_df` before filtering, perform the filtering, and finally display the number of unique stations after filtering to show the effect of the operation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a020638",
        "outputId": "9457576e-c24d-4ad8-c073-7ff215c5b389"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. รับค่า Node ทั้งหมดจาก DataFrame stations\n",
        "station_nodes = stations['Node'].unique()\n",
        "\n",
        "# 2. แสดงจำนวนสถานีที่ไม่ซ้ำกันใน operational_daily_rainfall_df ก่อนการกรอง\n",
        "initial_unique_stations = operational_daily_rainfall_df['tele_station_oldcode'].nunique()\n",
        "print(f\"จำนวนสถานีที่ไม่ซ้ำกันใน operational_daily_rainfall_df ก่อนการกรอง: {initial_unique_stations}\")\n",
        "\n",
        "# 3. กรอง operational_daily_rainfall_df โดยเลือกเฉพาะแถวที่คอลัมน์ tele_station_oldcode มีค่าตรงกับ Node ที่อยู่ใน DataFrame stations\n",
        "filtered_operational_df = operational_daily_rainfall_df[operational_daily_rainfall_df['tele_station_oldcode'].isin(station_nodes)].copy()\n",
        "\n",
        "# 4. แสดงจำนวนสถานีที่ไม่ซ้ำกันใน operational_daily_rainfall_df หลังจากที่ได้กรองแล้ว\n",
        "final_unique_stations = filtered_operational_df['tele_station_oldcode'].nunique()\n",
        "print(f\"จำนวนสถานีที่ไม่ซ้ำกันใน operational_daily_rainfall_df หลังการกรอง: {final_unique_stations}\")\n",
        "\n",
        "print(\"Filtering completed. Displaying head of filtered_operational_df for inspection.\")\n",
        "print(filtered_operational_df.head())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "จำนวนสถานีที่ไม่ซ้ำกันใน operational_daily_rainfall_df ก่อนการกรอง: 1514\n",
            "จำนวนสถานีที่ไม่ซ้ำกันใน operational_daily_rainfall_df หลังการกรอง: 384\n",
            "Filtering completed. Displaying head of filtered_operational_df for inspection.\n",
            "            rainfall_datetime     rainfall_datetime_calc  rainfall_value  \\\n",
            "1   2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "9   2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "10  2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "13  2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "169 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "\n",
            "     tele_station_id tele_station_oldcode measure_datetime  \n",
            "1              135.0               VLGE03       2025-12-02  \n",
            "9               43.0                 NOKK       2025-12-02  \n",
            "10             143.0                 NBON       2025-12-02  \n",
            "13             119.0                 BNGR       2025-12-02  \n",
            "169            939.0                 BKHL       2025-12-02  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b73bf199"
      },
      "source": [
        "## Identify Missing Records\n",
        "\n",
        "### Subtask:\n",
        "ระบุ 'ข้อมูลสูญหาย' ในข้อมูลปริมาณน้ำฝนรายวัน (Operational Daily Rainfall Data) ซึ่งหมายถึงวันที่ไม่มีบันทึกข้อมูลเลย (ไม่ใช่ค่าว่างหรือ Null) สำหรับแต่ละสถานี โดยการตรวจสอบช่วงวันที่ของข้อมูลที่มีอยู่และสร้างช่วงวันที่ที่สมบูรณ์ จากนั้นคำนวณจำนวนและสัดส่วนของข้อมูลที่สูญหายสำหรับแต่ละสถานีและแสดงผล\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b6580e"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to rename the 'tele_station_oldcode' column to 'station_code' in the `filtered_operational_df` for consistency with other dataframes and to prepare for grouping operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be8f9f3a",
        "outputId": "9f99ce22-02f3-441d-b095-5b0789f36156"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. เปลี่ยนชื่อคอลัมน์ 'tele_station_oldcode' ใน filtered_operational_df เป็น 'station_code'\n",
        "filtered_operational_df = filtered_operational_df.rename(columns={'tele_station_oldcode': 'station_code'})\n",
        "\n",
        "print(\"Column 'tele_station_oldcode' renamed to 'station_code'.\")\n",
        "print(\"Updated columns:\", filtered_operational_df.columns)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'tele_station_oldcode' renamed to 'station_code'.\n",
            "Updated columns: Index(['rainfall_datetime', 'rainfall_datetime_calc', 'rainfall_value',\n",
            "       'tele_station_id', 'station_code', 'measure_datetime'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9a6b121"
      },
      "source": [
        "**Reasoning**:\n",
        "I will group the data by station, determine the min and max dates, generate a complete date range, identify actual recorded dates, and then calculate missing records count and percentage for each station, finally displaying the results as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2616c91f",
        "outputId": "9af732be-cb7b-4f0d-d880-dbf211e13771"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure 'rainfall_datetime' is a datetime object\n",
        "filtered_operational_df['rainfall_datetime'] = pd.to_datetime(filtered_operational_df['rainfall_datetime'])\n",
        "\n",
        "# 2. สำหรับแต่ละสถานีใน filtered_operational_df ให้ระบุช่วงวันที่ที่มีข้อมูลอยู่จริง (วันเริ่มต้นและวันสิ้นสุด)\n",
        "station_date_ranges = filtered_operational_df.groupby('station_code')['rainfall_datetime'].agg(['min', 'max']).reset_index()\n",
        "station_date_ranges.rename(columns={'min': 'start_date', 'max': 'end_date'}, inplace=True)\n",
        "\n",
        "missing_data_summary = []\n",
        "\n",
        "# 3. สร้างช่วงวันที่ที่สมบูรณ์สำหรับแต่ละสถานี\n",
        "# 4. เปรียบเทียบวันที่ที่มีอยู่จริงกับช่วงวันที่ที่สมบูรณ์ เพื่อระบุวันที่ที่ขาดหายไป\n",
        "# 5. คำนวณจำนวนข้อมูลที่ขาดหายไปและสัดส่วนของข้อมูลที่ขาดหายไป\n",
        "for index, row in station_date_ranges.iterrows():\n",
        "    station_code = row['station_code']\n",
        "    start_date = row['start_date']\n",
        "    end_date = row['end_date']\n",
        "\n",
        "    # Generate a complete daily date range for the station\n",
        "    full_date_range = pd.date_range(start=start_date.normalize(), end=end_date.normalize(), freq='D')\n",
        "    total_expected_days = len(full_date_range)\n",
        "\n",
        "    # Get actual recorded dates for the current station\n",
        "    actual_recorded_dates = filtered_operational_df[filtered_operational_df['station_code'] == station_code]['rainfall_datetime'].dt.normalize().unique()\n",
        "    actual_recorded_days = len(actual_recorded_dates)\n",
        "\n",
        "    # Calculate missing days\n",
        "    missing_records_count = total_expected_days - actual_recorded_days\n",
        "    missing_records_percentage = (missing_records_count / total_expected_days) * 100 if total_expected_days > 0 else 0\n",
        "\n",
        "    missing_data_summary.append({\n",
        "        'station_code': station_code,\n",
        "        'total_expected_days': total_expected_days,\n",
        "        'actual_recorded_days': actual_recorded_days,\n",
        "        'missing_records_count': missing_records_count,\n",
        "        'missing_records_percentage': missing_records_percentage\n",
        "    })\n",
        "\n",
        "# 6. สร้าง DataFrame ใหม่เพื่อเก็บผลลัพธ์\n",
        "missing_data_df = pd.DataFrame(missing_data_summary)\n",
        "\n",
        "# 7. แสดง 5 แถวแรกของ DataFrame ผลลัพธ์\n",
        "print(\"\\n5 แถวแรกของ DataFrame สรุปข้อมูลสูญหาย (missing_data_df):\\n\")\n",
        "print(missing_data_df.head())\n",
        "\n",
        "# แสดงข้อมูลสรุปทางสถิติของคอลัมน์ 'missing_records_percentage'\n",
        "print(\"\\nค่าสถิติเชิงพรรณนาของ 'missing_records_percentage':\\n\")\n",
        "print(missing_data_df['missing_records_percentage'].describe())\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 แถวแรกของ DataFrame สรุปข้อมูลสูญหาย (missing_data_df):\n",
            "\n",
            "  station_code  total_expected_days  actual_recorded_days  \\\n",
            "0         ACRU                  184                   184   \n",
            "1         BBHN                  184                   184   \n",
            "2         BBUA                  184                   169   \n",
            "3         BCNG                  184                   184   \n",
            "4         BDAR                  184                   184   \n",
            "\n",
            "   missing_records_count  missing_records_percentage  \n",
            "0                      0                    0.000000  \n",
            "1                      0                    0.000000  \n",
            "2                     15                    8.152174  \n",
            "3                      0                    0.000000  \n",
            "4                      0                    0.000000  \n",
            "\n",
            "ค่าสถิติเชิงพรรณนาของ 'missing_records_percentage':\n",
            "\n",
            "count    384.000000\n",
            "mean       2.520604\n",
            "std        7.941346\n",
            "min        0.000000\n",
            "25%        0.000000\n",
            "50%        0.000000\n",
            "75%        0.543478\n",
            "max       66.847826\n",
            "Name: missing_records_percentage, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7abff81"
      },
      "source": [
        "## Remove Stations with High Missing Data\n",
        "\n",
        "### Subtask:\n",
        "คัดสถานีที่มีข้อมูลสูญหาย (ตามที่ระบุในขั้นตอนก่อนหน้า) มากกว่าหรือเท่ากับ 20% ออกจากชุดข้อมูลทั้งหมด โดยจะแสดงรายชื่อสถานีที่ถูกคัดออกและจำนวนสถานีที่เหลืออยู่\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ad2705"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to identify stations with more than or equal to 20% missing data from `missing_data_df`, then filter `filtered_operational_df` based on these stations, and finally display the affected stations and the counts before and after filtering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa19903d",
        "outputId": "df2cfd0d-2429-4a3f-f52a-8a1037c4f5a6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. ระบุรหัสสถานี ('station_code') ที่มีค่า 'missing_records_percentage' มากกว่าหรือเท่ากับ 20% จาก DataFrame `missing_data_df`\n",
        "stations_to_remove_df = missing_data_df[missing_data_df['missing_records_percentage'] >= 20]\n",
        "stations_to_remove = stations_to_remove_df['station_code'].tolist()\n",
        "\n",
        "# 2. แสดงรายชื่อสถานีที่ถูกคัดออก (สถานีใน `stations_to_remove`)\n",
        "print(f\"\\nรายชื่อสถานีที่ถูกคัดออกเนื่องจากมีข้อมูลสูญหาย >= 20%: {stations_to_remove}\")\n",
        "print(f\"จำนวนสถานีที่ถูกคัดออก: {len(stations_to_remove)}\")\n",
        "\n",
        "# 3. แสดงจำนวนสถานีก่อนการคัดออก (จำนวนสถานีที่ไม่ซ้ำกันใน `filtered_operational_df`)\n",
        "initial_stations_count = filtered_operational_df['station_code'].nunique()\n",
        "print(f\"\\nจำนวนสถานีที่ไม่ซ้ำกันในชุดข้อมูลก่อนการคัดออก: {initial_stations_count}\")\n",
        "\n",
        "# 4. กรอง DataFrame `filtered_operational_df` เพื่อลบแถวที่ 'station_code' อยู่ใน `stations_to_remove` ออก\n",
        "processed_operational_df = filtered_operational_df[~filtered_operational_df['station_code'].isin(stations_to_remove)].copy()\n",
        "\n",
        "# 5. แสดงจำนวนสถานีที่เหลืออยู่หลังจากคัดออก (จำนวนสถานีที่ไม่ซ้ำกันใน `processed_operational_df`)\n",
        "final_stations_count = processed_operational_df['station_code'].nunique()\n",
        "print(f\"จำนวนสถานีที่ไม่ซ้ำกันในชุดข้อมูลหลังการคัดออก: {final_stations_count}\")\n",
        "\n",
        "# 6. แสดง 5 แถวแรกของ `processed_operational_df` เพื่อตรวจสอบ\n",
        "print(\"\\n5 แถวแรกของ DataFrame `processed_operational_df` หลังการคัดออกสถานี:\\n\")\n",
        "print(processed_operational_df.head())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "รายชื่อสถานีที่ถูกคัดออกเนื่องจากมีข้อมูลสูญหาย >= 20%: ['BKHN', 'BLKO', 'BNLG', 'BSPN', 'CGKM', 'CHI010', 'CHI013', 'CPY010', 'CTKN', 'KKTO', 'MMMO', 'MNRM', 'NEAK', 'NGCG', 'NGPI', 'NKSG', 'SGKW', 'SKE001', 'SKM004', 'SLA002', 'TPI007', 'TUKW']\n",
            "จำนวนสถานีที่ถูกคัดออก: 22\n",
            "\n",
            "จำนวนสถานีที่ไม่ซ้ำกันในชุดข้อมูลก่อนการคัดออก: 384\n",
            "จำนวนสถานีที่ไม่ซ้ำกันในชุดข้อมูลหลังการคัดออก: 362\n",
            "\n",
            "5 แถวแรกของ DataFrame `processed_operational_df` หลังการคัดออกสถานี:\n",
            "\n",
            "            rainfall_datetime     rainfall_datetime_calc  rainfall_value  \\\n",
            "1   2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "9   2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "10  2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "13  2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "169 2025-12-03 00:00:00+07:00  2025-12-02T07:00:00+07:00             0.0   \n",
            "\n",
            "     tele_station_id station_code measure_datetime  \n",
            "1              135.0       VLGE03       2025-12-02  \n",
            "9               43.0         NOKK       2025-12-02  \n",
            "10             143.0         NBON       2025-12-02  \n",
            "13             119.0         BNGR       2025-12-02  \n",
            "169            939.0         BKHL       2025-12-02  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99bba8e"
      },
      "source": [
        "## Impute Missing Data with Median\n",
        "\n",
        "### Subtask:\n",
        "เติมข้อมูลที่สูญหายในข้อมูลปริมาณน้ำฝนรายวันของสถานีที่เหลืออยู่ โดยใช้ค่า Median ของ 'Day of Year' จากข้อมูลน้ำฝนรายวันย้อนหลัง (hii_daily_rain.csv) แยกตามสถานี หากสถานีนั้นไม่มีข้อมูลใน historical จะใช้ค่า median จาก DoY ภาพรวม จากนั้นจะตรวจสอบอีกครั้งเพื่อให้แน่ใจว่าไม่มี record ที่สูญหายอีกต่อไป\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77c9713e"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction requires renaming columns in `historical_df` for consistency and converting the datetime column to date-only format. This aligns the historical data with the target format for imputation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1363220",
        "outputId": "9126880f-5c7e-45e3-9cf1-ebc482a395ec"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. เปลี่ยนชื่อคอลัมน์ใน `historical_df` ให้เป็นมาตรฐาน: เปลี่ยน `measure_datetime` เป็น `date` และ `data` เป็น `rainfall_value`\n",
        "# พร้อมทั้งแปลงคอลัมน์ `date` ให้เป็นประเภท datetime และตัดเวลาออกให้เหลือแต่วันที่\n",
        "\n",
        "historical_df_cleaned = historical_df.copy()\n",
        "historical_df_cleaned = historical_df_cleaned.rename(columns={'measure_datetime': 'date', 'data': 'rainfall_value'})\n",
        "\n",
        "historical_df_cleaned['date'] = pd.to_datetime(historical_df_cleaned['date']).dt.normalize()\n",
        "\n",
        "print(\"Historical DataFrame columns renamed and date column normalized.\")\n",
        "print(\"New columns:\", historical_df_cleaned.columns)\n",
        "print(\"5 แถวแรกของ historical_df_cleaned หลังปรับปรุง:\")\n",
        "print(historical_df_cleaned.head())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Historical DataFrame columns renamed and date column normalized.\n",
            "New columns: Index(['station_code', 'date', 'rainfall_value', 'station_name', 'latitude',\n",
            "       'longitude', 'quality_flag', 'basin', 'sub_basin', 'tambon', 'amphoe',\n",
            "       'province', 'data_type'],\n",
            "      dtype='object')\n",
            "5 แถวแรกของ historical_df_cleaned หลังปรับปรุง:\n",
            "  station_code                      date  rainfall_value station_name  \\\n",
            "0         ABRT 2012-01-01 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "1         ABRT 2012-01-03 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "2         ABRT 2012-01-06 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "3         ABRT 2012-01-07 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "4         ABRT 2012-01-08 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "\n",
            "    latitude  longitude quality_flag basin           sub_basin      tambon  \\\n",
            "0  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "1  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "2  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "3  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "4  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "\n",
            "          amphoe  province       data_type  \n",
            "0  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "1  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "2  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "3  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n",
            "4  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57d2b7f0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `historical_df_cleaned` has consistent column names and normalized dates, I will extract the 'DayOfYear' to prepare for calculating median rainfall values for imputation. This is instruction #2 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbbe16c8",
        "outputId": "37d51469-d48d-4be7-af46-26cb6c6cd331"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 2. สร้างคอลัมน์ `DayOfYear` ใน `historical_df_cleaned` โดยดึงค่าวันในรอบปี (Day of Year) ออกมาจากคอลัมน์ `date`.\n",
        "historical_df_cleaned['DayOfYear'] = historical_df_cleaned['date'].dt.dayofyear\n",
        "\n",
        "print(\"Created 'DayOfYear' column in historical_df_cleaned.\")\n",
        "print(\"5 แถวแรกของ historical_df_cleaned พร้อมคอลัมน์ DayOfYear:\")\n",
        "print(historical_df_cleaned.head())"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'DayOfYear' column in historical_df_cleaned.\n",
            "5 แถวแรกของ historical_df_cleaned พร้อมคอลัมน์ DayOfYear:\n",
            "  station_code                      date  rainfall_value station_name  \\\n",
            "0         ABRT 2012-01-01 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "1         ABRT 2012-01-03 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "2         ABRT 2012-01-06 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "3         ABRT 2012-01-07 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "4         ABRT 2012-01-08 00:00:00+07:00             0.0   วัดเวฬุวัน   \n",
            "\n",
            "    latitude  longitude quality_flag basin           sub_basin      tambon  \\\n",
            "0  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "1  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "2  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "3  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "4  16.054785  103.66429            U    ชี  ลำน้ำชีส่วนที่ 4/2  เหนือเมือง   \n",
            "\n",
            "          amphoe  province       data_type  DayOfYear  \n",
            "0  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily          1  \n",
            "1  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily          3  \n",
            "2  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily          6  \n",
            "3  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily          7  \n",
            "4  เมืองร้อยเอ็ด  ร้อยเอ็ด  rainfall_daily          8  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec47dd5e"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the subtask instructions, I will now calculate the median `rainfall_value` from `historical_df_cleaned`, grouped by `station_code` and `DayOfYear`, and store the result in a new DataFrame called `station_doy_medians`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c1512d3",
        "outputId": "0d0088c3-2f96-4c42-a88e-1230773250d6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 3. คำนวณค่ามัธยฐาน (median) ของ `rainfall_value` จาก `historical_df_cleaned`\n",
        "# โดยจัดกลุ่มตาม `station_code` และ `DayOfYear` แล้วเก็บผลลัพธ์ไว้ใน DataFrame ใหม่ชื่อ `station_doy_medians`.\n",
        "station_doy_medians = historical_df_cleaned.groupby(['station_code', 'DayOfYear'])['rainfall_value'].median().reset_index()\n",
        "\n",
        "print(\"Calculated station-specific DayOfYear medians.\")\n",
        "print(\"5 แถวแรกของ station_doy_medians:\")\n",
        "print(station_doy_medians.head())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated station-specific DayOfYear medians.\n",
            "5 แถวแรกของ station_doy_medians:\n",
            "  station_code  DayOfYear  rainfall_value\n",
            "0         ABRT          1             0.0\n",
            "1         ABRT          2             0.0\n",
            "2         ABRT          3             0.0\n",
            "3         ABRT          4             0.0\n",
            "4         ABRT          5             0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d692b76"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #4 of the subtask, I will calculate the overall median rainfall value from `historical_df_cleaned` by grouping only by `DayOfYear`. This will serve as a fallback imputation value for stations that might not have station-specific historical data for a particular DayOfYear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "941bfda3",
        "outputId": "5c206f20-bb28-4f41-c9c6-976ab5710165"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 4. คำนวณค่ามัธยฐาน (median) ของ `rainfall_value` จาก `historical_df_cleaned`\n",
        "# โดยจัดกลุ่มเฉพาะตาม `DayOfYear` (สำหรับใช้เป็นค่า fallback) แล้วเก็บผลลัพธ์ไว้ใน DataFrame ใหม่ชื่อ `overall_doy_medians`.\n",
        "overall_doy_medians = historical_df_cleaned.groupby('DayOfYear')['rainfall_value'].median().reset_index()\n",
        "\n",
        "print(\"Calculated overall DayOfYear medians (fallback values).\")\n",
        "print(\"5 แถวแรกของ overall_doy_medians:\")\n",
        "print(overall_doy_medians.head())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated overall DayOfYear medians (fallback values).\n",
            "5 แถวแรกของ overall_doy_medians:\n",
            "   DayOfYear  rainfall_value\n",
            "0          1             0.0\n",
            "1          2             0.0\n",
            "2          3             0.0\n",
            "3          4             0.0\n",
            "4          5             0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5c6d28"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #5, I need to create a DataFrame `all_expected_records_df` that contains every date for each station present in `processed_operational_df`, using the start and end dates derived from `missing_data_df` (after filtering out stations that were removed). This will establish a complete timeline for each active station.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "ef9be7af",
        "outputId": "18d266d4-1fa4-45a3-f118-e46c04e95c2c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 5. สร้าง DataFrame `all_expected_records_df`\n",
        "\n",
        "# Get the list of station codes that are still in processed_operational_df\n",
        "active_station_codes = processed_operational_df['station_code'].unique()\n",
        "\n",
        "# Filter missing_data_df to get start and end dates only for active stations\n",
        "active_station_date_ranges = missing_data_df[missing_data_df['station_code'].isin(active_station_codes)]\n",
        "\n",
        "# Initialize a list to store data for all expected records\n",
        "all_expected_records = []\n",
        "\n",
        "for index, row in active_station_date_ranges.iterrows():\n",
        "    station_code = row['station_code']\n",
        "    start_date = row['start_date'].normalize() # Ensure date-only for consistency\n",
        "    end_date = row['end_date'].normalize() # Ensure date-only for consistency\n",
        "\n",
        "    # Generate a complete daily date range for the current station\n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "    # Create a temporary DataFrame for this station's full date range\n",
        "    temp_df = pd.DataFrame({\n",
        "        'station_code': station_code,\n",
        "        'date': date_range\n",
        "    })\n",
        "    all_expected_records.append(temp_df)\n",
        "\n",
        "# Concatenate all temporary DataFrames into all_expected_records_df\n",
        "if all_expected_records:\n",
        "    all_expected_records_df = pd.concat(all_expected_records, ignore_index=True)\n",
        "else:\n",
        "    all_expected_records_df = pd.DataFrame(columns=['station_code', 'date'])\n",
        "\n",
        "print(\"Created all_expected_records_df with complete date ranges for active stations.\")\n",
        "print(\"5 แถวแรกของ all_expected_records_df:\")\n",
        "print(all_expected_records_df.head())\n",
        "print(\"ข้อมูลสรุปของ all_expected_records_df:\")\n",
        "all_expected_records_df.info()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2072646461.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive_station_date_ranges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstation_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'station_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ensure date-only for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ensure date-only for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_date'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ebafc6"
      },
      "source": [
        "**Reasoning**:\n",
        "The `KeyError: 'start_date'` occurred because the `active_station_date_ranges` DataFrame, which was derived from `missing_data_df`, did not contain the 'start_date' and 'end_date' columns. These columns were present in `station_date_ranges`. To fix this, I will filter the `station_date_ranges` DataFrame directly using the `active_station_codes` to obtain the correct start and end dates for each active station.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8b7d1ff",
        "outputId": "73526921-aa1a-4e25-fb00-e0fd95fe96bb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 5. สร้าง DataFrame `all_expected_records_df`\n",
        "\n",
        "# Get the list of station codes that are still in processed_operational_df\n",
        "active_station_codes = processed_operational_df['station_code'].unique()\n",
        "\n",
        "# Filter station_date_ranges to get start and end dates only for active stations\n",
        "# This ensures 'start_date' and 'end_date' columns are present\n",
        "active_station_date_ranges = station_date_ranges[station_date_ranges['station_code'].isin(active_station_codes)]\n",
        "\n",
        "# Initialize a list to store data for all expected records\n",
        "all_expected_records = []\n",
        "\n",
        "for index, row in active_station_date_ranges.iterrows():\n",
        "    station_code = row['station_code']\n",
        "    start_date = row['start_date'].normalize() # Ensure date-only for consistency\n",
        "    end_date = row['end_date'].normalize() # Ensure date-only for consistency\n",
        "\n",
        "    # Generate a complete daily date range for the current station\n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "    # Create a temporary DataFrame for this station's full date range\n",
        "    temp_df = pd.DataFrame({\n",
        "        'station_code': station_code,\n",
        "        'date': date_range\n",
        "    })\n",
        "    all_expected_records.append(temp_df)\n",
        "\n",
        "# Concatenate all temporary DataFrames into all_expected_records_df\n",
        "if all_expected_records:\n",
        "    all_expected_records_df = pd.concat(all_expected_records, ignore_index=True)\n",
        "else:\n",
        "    all_expected_records_df = pd.DataFrame(columns=['station_code', 'date'])\n",
        "\n",
        "print(\"Created all_expected_records_df with complete date ranges for active stations.\")\n",
        "print(\"5 แถวแรกของ all_expected_records_df:\")\n",
        "print(all_expected_records_df.head())\n",
        "print(\"ข้อมูลสรุปของ all_expected_records_df:\")\n",
        "all_expected_records_df.info()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created all_expected_records_df with complete date ranges for active stations.\n",
            "5 แถวแรกของ all_expected_records_df:\n",
            "  station_code                      date\n",
            "0         ACRU 2025-07-02 00:00:00+07:00\n",
            "1         ACRU 2025-07-03 00:00:00+07:00\n",
            "2         ACRU 2025-07-04 00:00:00+07:00\n",
            "3         ACRU 2025-07-05 00:00:00+07:00\n",
            "4         ACRU 2025-07-06 00:00:00+07:00\n",
            "ข้อมูลสรุปของ all_expected_records_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64097 entries, 0 to 64096\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype                    \n",
            "---  ------        --------------  -----                    \n",
            " 0   station_code  64097 non-null  object                   \n",
            " 1   date          64097 non-null  datetime64[ns, UTC+07:00]\n",
            "dtypes: datetime64[ns, UTC+07:00](1), object(1)\n",
            "memory usage: 1001.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae3848ba"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #6, I will merge the `all_expected_records_df` with the `processed_operational_df` using `station_code` and the date column. Before merging, I will rename the `rainfall_datetime` column in `processed_operational_df` to `date` to ensure consistent column names for the merge operation, and also normalize the datetime to date-only to match `all_expected_records_df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0b088cb",
        "outputId": "a56df579-d7fc-4c14-c0d7-16db9cd5a417"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 6. ผสาน (merge) `all_expected_records_df` เข้ากับ `processed_operational_df`\n",
        "\n",
        "# Make a copy and rename the datetime column in processed_operational_df for merging consistency\n",
        "operational_df_for_merge = processed_operational_df.copy()\n",
        "operational_df_for_merge = operational_df_for_merge.rename(columns={'rainfall_datetime': 'date'})\n",
        "operational_df_for_merge['date'] = operational_df_for_merge['date'].dt.normalize()\n",
        "\n",
        "# Perform a left merge to ensure all expected records are present.\n",
        "# The 'rainfall_value' will be NaN for dates where there was no original data.\n",
        "merged_df = pd.merge(\n",
        "    all_expected_records_df,\n",
        "    operational_df_for_merge[['station_code', 'date', 'rainfall_value']],\n",
        "    on=['station_code', 'date'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(\"Merged all_expected_records_df with processed_operational_df.\")\n",
        "print(\"5 แถวแรกของ merged_df:\")\n",
        "print(merged_df.head())\n",
        "print(\"จำนวนค่า NaN ในคอลัมน์ 'rainfall_value' หลังการผสาน:\", merged_df['rainfall_value'].isnull().sum())\n",
        "print(\"ข้อมูลสรุปของ merged_df:\")\n",
        "merged_df.info()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged all_expected_records_df with processed_operational_df.\n",
            "5 แถวแรกของ merged_df:\n",
            "  station_code                      date  rainfall_value\n",
            "0         ACRU 2025-07-02 00:00:00+07:00             0.0\n",
            "1         ACRU 2025-07-03 00:00:00+07:00             0.0\n",
            "2         ACRU 2025-07-04 00:00:00+07:00             0.8\n",
            "3         ACRU 2025-07-05 00:00:00+07:00             0.0\n",
            "4         ACRU 2025-07-06 00:00:00+07:00             9.8\n",
            "จำนวนค่า NaN ในคอลัมน์ 'rainfall_value' หลังการผสาน: 485\n",
            "ข้อมูลสรุปของ merged_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64097 entries, 0 to 64096\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype                    \n",
            "---  ------          --------------  -----                    \n",
            " 0   station_code    64097 non-null  object                   \n",
            " 1   date            64097 non-null  datetime64[ns, UTC+07:00]\n",
            " 2   rainfall_value  63612 non-null  float64                  \n",
            "dtypes: datetime64[ns, UTC+07:00](1), float64(1), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c06c94c2"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #7, I will impute the missing rainfall values (NaNs) in `merged_df` using the calculated medians. This will be done in a multi-step approach: first, using station-specific DayOfYear medians, then falling back to overall DayOfYear medians, and finally filling any remaining NaNs with 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a85e3eb",
        "outputId": "a7a59e86-63b9-4acd-ece3-14dbeab86edb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 7. เติมค่าที่สูญหาย (NaN) ในคอลัมน์ `rainfall_value` ของ DataFrame ที่ผสานแล้ว:\n",
        "\n",
        "# 7a. เพิ่มคอลัมน์ DayOfYear ใน merged_df เพื่อใช้ในการผสานกับข้อมูล median\n",
        "merged_df['DayOfYear'] = merged_df['date'].dt.dayofyear\n",
        "\n",
        "# 7a. ผสาน DataFrame กับ `station_doy_medians` เพื่อดึงค่ามัธยฐานเฉพาะสถานีและวันในรอบปีมาใช้เติม.\n",
        "# Rename 'rainfall_value' in station_doy_medians to 'station_median_rainfall' to avoid column name conflict during merge\n",
        "station_doy_medians_renamed = station_doy_medians.rename(columns={'rainfall_value': 'station_median_rainfall'})\n",
        "\n",
        "# Merge to get station-specific median rainfall values\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    station_doy_medians_renamed[['station_code', 'DayOfYear', 'station_median_rainfall']],\n",
        "    on=['station_code', 'DayOfYear'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill NaN values in 'rainfall_value' using 'station_median_rainfall'\n",
        "merged_df['rainfall_value'].fillna(merged_df['station_median_rainfall'], inplace=True)\n",
        "\n",
        "# 7b. เติมค่า `NaN` ที่เหลือ (หากไม่มีค่ามัธยฐานเฉพาะสถานี) ด้วยค่ามัธยฐานรวมตามวันในรอบปีจาก `overall_doy_medians`.\n",
        "# Rename 'rainfall_value' in overall_doy_medians to 'overall_median_rainfall'\n",
        "overall_doy_medians_renamed = overall_doy_medians.rename(columns={'rainfall_value': 'overall_median_rainfall'})\n",
        "\n",
        "# Merge to get overall median rainfall values for remaining NaNs\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    overall_doy_medians_renamed[['DayOfYear', 'overall_median_rainfall']],\n",
        "    on='DayOfYear',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill remaining NaN values in 'rainfall_value' using 'overall_median_rainfall'\n",
        "merged_df['rainfall_value'].fillna(merged_df['overall_median_rainfall'], inplace=True)\n",
        "\n",
        "# 7c. เติมค่า `NaN` ที่ยังเหลืออยู่ทั้งหมด (หากไม่พบค่ามัธยฐานใดๆ) ด้วยค่า 0.\n",
        "merged_df['rainfall_value'].fillna(0, inplace=True)\n",
        "\n",
        "# Drop the temporary median columns\n",
        "merged_df.drop(columns=['DayOfYear', 'station_median_rainfall', 'overall_median_rainfall'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"Missing rainfall values imputed using median of DayOfYear from historical data.\")\n",
        "print(\"5 แถวแรกของ merged_df หลังการเติมข้อมูล:\")\n",
        "print(merged_df.head())\n",
        "print(\"จำนวนค่า NaN ในคอลัมน์ 'rainfall_value' หลังการเติมข้อมูล:\", merged_df['rainfall_value'].isnull().sum())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing rainfall values imputed using median of DayOfYear from historical data.\n",
            "5 แถวแรกของ merged_df หลังการเติมข้อมูล:\n",
            "  station_code                      date  rainfall_value\n",
            "0         ACRU 2025-07-02 00:00:00+07:00             0.0\n",
            "1         ACRU 2025-07-03 00:00:00+07:00             0.0\n",
            "2         ACRU 2025-07-04 00:00:00+07:00             0.8\n",
            "3         ACRU 2025-07-05 00:00:00+07:00             0.0\n",
            "4         ACRU 2025-07-06 00:00:00+07:00             9.8\n",
            "จำนวนค่า NaN ในคอลัมน์ 'rainfall_value' หลังการเติมข้อมูล: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-898752855.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_df['rainfall_value'].fillna(merged_df['station_median_rainfall'], inplace=True)\n",
            "/tmp/ipython-input-898752855.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_df['rainfall_value'].fillna(merged_df['overall_median_rainfall'], inplace=True)\n",
            "/tmp/ipython-input-898752855.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_df['rainfall_value'].fillna(0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "141bb1bc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code generated `FutureWarning` messages due to the use of `inplace=True` on chained assignments. To resolve this, I will modify the `.fillna()` calls to use direct assignment instead of `inplace=True` and ensure explicit verification of no remaining NaN values as per the subtask instruction 8.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84ff48fe",
        "outputId": "b50b0a89-7c21-4ac4-80df-8576fce71569"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 7. เติมค่าที่สูญหาย (NaN) ในคอลัมน์ `rainfall_value` ของ DataFrame ที่ผสานแล้ว:\n",
        "\n",
        "# 7a. เพิ่มคอลัมน์ DayOfYear ใน merged_df เพื่อใช้ในการผสานกับข้อมูล median\n",
        "merged_df['DayOfYear'] = merged_df['date'].dt.dayofyear\n",
        "\n",
        "# 7a. ผสาน DataFrame กับ `station_doy_medians` เพื่อดึงค่ามัธยฐานเฉพาะสถานีและวันในรอบปีมาใช้เติม.\n",
        "# Rename 'rainfall_value' in station_doy_medians to 'station_median_rainfall' to avoid column name conflict during merge\n",
        "station_doy_medians_renamed = station_doy_medians.rename(columns={'rainfall_value': 'station_median_rainfall'})\n",
        "\n",
        "# Merge to get station-specific median rainfall values\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    station_doy_medians_renamed[['station_code', 'DayOfYear', 'station_median_rainfall']],\n",
        "    on=['station_code', 'DayOfYear'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill NaN values in 'rainfall_value' using 'station_median_rainfall' with direct assignment\n",
        "merged_df['rainfall_value'] = merged_df['rainfall_value'].fillna(merged_df['station_median_rainfall'])\n",
        "\n",
        "# 7b. เติมค่า `NaN` ที่เหลือ (หากไม่มีค่ามัธยฐานเฉพาะสถานี) ด้วยค่ามัธยฐานรวมตามวันในรอบปีจาก `overall_doy_medians`.\n",
        "# Rename 'rainfall_value' in overall_doy_medians to 'overall_median_rainfall'\n",
        "overall_doy_medians_renamed = overall_doy_medians.rename(columns={'rainfall_value': 'overall_median_rainfall'})\n",
        "\n",
        "# Merge to get overall median rainfall values for remaining NaNs\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    overall_doy_medians_renamed[['DayOfYear', 'overall_median_rainfall']],\n",
        "    on='DayOfYear',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill remaining NaN values in 'rainfall_value' using 'overall_median_rainfall' with direct assignment\n",
        "merged_df['rainfall_value'] = merged_df['rainfall_value'].fillna(merged_df['overall_median_rainfall'])\n",
        "\n",
        "# 7c. เติมค่า `NaN` ที่ยังเหลืออยู่ทั้งหมด (หากไม่พบค่ามัธยฐานใดๆ) ด้วยค่า 0.\n",
        "merged_df['rainfall_value'] = merged_df['rainfall_value'].fillna(0)\n",
        "\n",
        "# Drop the temporary median columns\n",
        "merged_df.drop(columns=['DayOfYear', 'station_median_rainfall', 'overall_median_rainfall'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"Missing rainfall values imputed using median of DayOfYear from historical data.\")\n",
        "print(\"5 แถวแรกของ merged_df หลังการเติมข้อมูล:\")\n",
        "print(merged_df.head())\n",
        "\n",
        "# 8. ตรวจสอบว่าไม่มีค่า `NaN` เหลืออยู่ในคอลัมน์ `rainfall_value` ของ DataFrame ที่ถูกเติมข้อมูลแล้ว\n",
        "print(\"\\nตรวจสอบค่า NaN ในคอลัมน์ 'rainfall_value' หลังการเติมข้อมูล: \")\n",
        "print(f\"จำนวนค่า NaN ในคอลัมน์ 'rainfall_value': {merged_df['rainfall_value'].isnull().sum()}\")\n",
        "\n",
        "if merged_df['rainfall_value'].isnull().sum() == 0:\n",
        "    print(\"ยืนยัน: ไม่มีค่า NaN เหลืออยู่ในคอลัมน์ 'rainfall_value'.\")\n",
        "else:\n",
        "    print(\"ข้อผิดพลาด: ยังมีค่า NaN เหลืออยู่ในคอลัมน์ 'rainfall_value'.\")\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing rainfall values imputed using median of DayOfYear from historical data.\n",
            "5 แถวแรกของ merged_df หลังการเติมข้อมูล:\n",
            "  station_code                      date  rainfall_value\n",
            "0         ACRU 2025-07-02 00:00:00+07:00             0.0\n",
            "1         ACRU 2025-07-03 00:00:00+07:00             0.0\n",
            "2         ACRU 2025-07-04 00:00:00+07:00             0.8\n",
            "3         ACRU 2025-07-05 00:00:00+07:00             0.0\n",
            "4         ACRU 2025-07-06 00:00:00+07:00             9.8\n",
            "\n",
            "ตรวจสอบค่า NaN ในคอลัมน์ 'rainfall_value' หลังการเติมข้อมูล: \n",
            "จำนวนค่า NaN ในคอลัมน์ 'rainfall_value': 0\n",
            "ยืนยัน: ไม่มีค่า NaN เหลืออยู่ในคอลัมน์ 'rainfall_value'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81d40f2b"
      },
      "source": [
        "## Explain Median vs Mean for Imputation\n",
        "\n",
        "### Subtask:\n",
        "อธิบายเหตุผลอย่างละเอียดว่าทำไมจึงเลือกใช้ค่า Median แทนค่า Mean ในการเติมข้อมูลที่สูญหาย โดยจะอธิบายถึงลักษณะของข้อมูลปริมาณน้ำฝนและผลกระทบของค่าผิดปกติ (outliers) ที่มีต่อค่าเฉลี่ย\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce6e37a"
      },
      "source": [
        "## Explain Median vs Mean for Imputation\n",
        "\n",
        "### Subtask:\n",
        "อธิบายเหตุผลอย่างละเอียดว่าทำไมจึงเลือกใช้ค่า Median แทนค่า Mean ในการเติมข้อมูลที่สูญหาย โดยจะอธิบายถึงลักษณะของข้อมูลปริมาณน้ำฝนและผลกระทบของค่าผิดปกติ (outliers) ที่มีต่อค่าเฉลี่ย\n",
        "\n",
        "### คำอธิบายเหตุผลในการเลือกใช้ค่า Median แทนค่า Mean ในการเติมข้อมูลปริมาณน้ำฝนที่สูญหาย:\n",
        "\n",
        "1.  **ลักษณะของข้อมูลปริมาณน้ำฝน**:\n",
        "    *   ข้อมูลปริมาณน้ำฝนมักมีการกระจายตัวแบบไม่สมมาตร (skewed distribution) ไปทางขวา นั่นคือจะมีค่าปริมาณน้ำฝนเป็นศูนย์ (ไม่มีฝนตก) หรือมีค่าน้อย ๆ เป็นจำนวนมากในแต่ละวัน แต่ในบางวันอาจมีค่าสูงมากจากการเกิดฝนตกหนักหรือพายุ\n",
        "    *   นอกจากนี้ ข้อมูลปริมาณน้ำฝนยังมีค่าเป็นศูนย์จำนวนมาก ซึ่งเป็นสถานการณ์ปกติที่ไม่มีฝนตกในแต่ละวัน\n",
        "\n",
        "2.  **แนวคิดของค่าผิดปกติ (Outliers) ในข้อมูลปริมาณน้ำฝน**:\n",
        "    *   ค่าผิดปกติในข้อมูลปริมาณน้ำฝนคือข้อมูลที่มีค่าสูงกว่าปกติอย่างมาก ซึ่งมักจะเกิดจากเหตุการณ์ฝนตกหนักผิดปกติ เช่น พายุฝนฟ้าคะนอง หรือพายุหมุนเขตร้อน ซึ่งค่าเหล่านี้แม้จะเกิดขึ้นไม่บ่อย แต่มีผลกระทบอย่างมากต่อค่าเฉลี่ย\n",
        "\n",
        "3.  **ผลกระทบของค่าผิดปกติที่มีต่อค่าเฉลี่ย (Mean)**:\n",
        "    *   ค่าเฉลี่ย (Mean) เป็นค่าที่ไวต่อค่าผิดปกติ (sensitive to outliers) อย่างมาก เนื่องจากค่าเฉลี่ยจะคำนวณโดยรวมค่าข้อมูลทุกค่าแล้วหารด้วยจำนวนข้อมูลทั้งหมด ดังนั้น หากมีค่าผิดปกติที่มีค่าสูงมากเพียงไม่กี่ค่า ค่าผิดปกติเหล่านั้นจะดึงค่าเฉลี่ยให้สูงขึ้นอย่างมีนัยสำคัญ ซึ่งอาจทำให้ค่าเฉลี่ยไม่สะท้อนค่ากลางส่วนใหญ่ของข้อมูลอย่างแท้จริง\n",
        "\n",
        "4.  **ความทนทานของค่ามัธยฐาน (Median) ต่อค่าผิดปกติ**:\n",
        "    *   ค่ามัธยฐาน (Median) คือค่ากลางของข้อมูลเมื่อนำข้อมูลมาเรียงลำดับจากน้อยไปมาก โดยจะเลือกค่าที่อยู่ตำแหน่งกลางพอดี ดังนั้น ค่ามัธยฐานจึงไม่ได้รับผลกระทบจากค่าผิดปกติมากเท่ากับค่าเฉลี่ย\n",
        "    *   ไม่ว่าจะมีค่าผิดปกติที่สูงหรือต่ำมากเพียงใด ค่ามัธยฐานจะยังคงแสดงถึงค่ากลางที่แท้จริงของข้อมูลส่วนใหญ่ได้ดีกว่า เนื่องจากไม่ได้นำขนาดของค่าผิดปกติมาคำนวณโดยตรง แต่เน้นที่ตำแหน่งของข้อมูล\n",
        "\n",
        "5.  **สรุปเหตุผลที่การใช้ค่ามัธยฐานเหมาะสมกว่า**:\n",
        "    *   เนื่องจากข้อมูลปริมาณน้ำฝนมีลักษณะการกระจายตัวที่ไม่สมมาตรและมีค่าผิดปกติ (ฝนตกหนัก) ที่สามารถส่งผลให้ค่าเฉลี่ยบิดเบือนไปจากความเป็นจริงได้ การใช้ค่ามัธยฐานจะช่วยให้ค่าที่นำมาเติมมีความน่าเชื่อถือและสะท้อนแนวโน้มของปริมาณน้ำฝนในวันนั้น ๆ ได้ดีกว่า โดยไม่ถูกดึงด้วยค่าฝนตกหนักผิดปกติที่ไม่ใช่สถานการณ์ทั่วไปของวันนั้น ๆ การใช้ Median จึงเป็นการรักษาลักษณะการกระจายตัวของข้อมูลเดิมได้ดีกว่าและลดอคติจากการเติมข้อมูลลง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df843496"
      },
      "source": [
        "## Calculate Monthly Rainfall and Format Output\n",
        "\n",
        "### Subtask:\n",
        "คำนวณผลรวมปริมาณน้ำฝนรายเดือนสำหรับแต่ละสถานีจากข้อมูลที่ได้รับการเติมเต็มแล้ว จัดรูปแบบข้อมูลผลลัพธ์ให้ตรงกับโครงสร้างของ 'Y_montly_rain_hii_operation.csv' ซึ่งอาจรวมถึงการจัดเรียงคอลัมน์และตั้งชื่อคอลัมน์ให้ตรงกัน\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb0fa4b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to extract 'year' and 'month' from the 'date' column in `merged_df` and then group the data by 'station_code', 'year', and 'month' to calculate the sum of 'rainfall_value' for monthly totals, as per instructions 1 and 2 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4654643e",
        "outputId": "f18c707a-847e-4ef3-d1ee-e24c930f7bf6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. จาก DataFrame `merged_df` ให้สร้างคอลัมน์ใหม่เพื่อเก็บปี ('year') และเดือน ('month') จากคอลัมน์ 'date'.\n",
        "monthly_rainfall_df = merged_df.copy()\n",
        "monthly_rainfall_df['year'] = monthly_rainfall_df['date'].dt.year\n",
        "monthly_rainfall_df['month'] = monthly_rainfall_df['date'].dt.month\n",
        "\n",
        "# 2. จัดกลุ่ม DataFrame `monthly_rainfall_df` ด้วย 'station_code', 'year', และ 'month'\n",
        "# แล้วคำนวณผลรวมของ 'rainfall_value' โดยใช้ `.sum()` และรีเซ็ตอินเด็กซ์.\n",
        "monthly_rainfall_summary = monthly_rainfall_df.groupby(['station_code', 'year', 'month'])['rainfall_value'].sum().reset_index()\n",
        "\n",
        "print(\"Created 'year' and 'month' columns and calculated monthly rainfall summary.\")\n",
        "print(\"5 แถวแรกของ monthly_rainfall_summary:\")\n",
        "print(monthly_rainfall_summary.head())\n",
        "print(\"ข้อมูลสรุปของ monthly_rainfall_summary:\")\n",
        "monthly_rainfall_summary.info()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'year' and 'month' columns and calculated monthly rainfall summary.\n",
            "5 แถวแรกของ monthly_rainfall_summary:\n",
            "  station_code  year  month  rainfall_value\n",
            "0         ACRU  2025      7            83.0\n",
            "1         ACRU  2025      8           164.6\n",
            "2         ACRU  2025      9           223.8\n",
            "3         ACRU  2025     10            72.0\n",
            "4         ACRU  2025     11            53.6\n",
            "ข้อมูลสรุปของ monthly_rainfall_summary:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2451 entries, 0 to 2450\n",
            "Data columns (total 4 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   station_code    2451 non-null   object \n",
            " 1   year            2451 non-null   int32  \n",
            " 2   month           2451 non-null   int32  \n",
            " 3   rainfall_value  2451 non-null   float64\n",
            "dtypes: float64(1), int32(2), object(1)\n",
            "memory usage: 57.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40abd2d"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to create a 'Month-Year' column in the `monthly_rainfall_summary` DataFrame by combining the 'year' and 'month' columns into 'YYYY-MM' format, as per instruction #3 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2245bd03",
        "outputId": "81f97bdc-42e5-481d-d8b5-91e3cbb3b6b0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 3. สร้างคอลัมน์ 'Month-Year' ใน DataFrame ผลรวม โดยรวมค่า 'year' และ 'month' เข้าด้วยกันให้อยู่ในรูปแบบ 'YYYY-MM'.\n",
        "monthly_rainfall_summary['Month-Year'] = monthly_rainfall_summary['year'].astype(str) + '-' + monthly_rainfall_summary['month'].astype(str).str.zfill(2)\n",
        "\n",
        "print(\"Created 'Month-Year' column.\")\n",
        "print(\"5 แถวแรกของ monthly_rainfall_summary พร้อมคอลัมน์ Month-Year:\")\n",
        "print(monthly_rainfall_summary.head())\n",
        "print(\"ข้อมูลสรุปของ monthly_rainfall_summary:\")\n",
        "monthly_rainfall_summary.info()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'Month-Year' column.\n",
            "5 แถวแรกของ monthly_rainfall_summary พร้อมคอลัมน์ Month-Year:\n",
            "  station_code  year  month  rainfall_value Month-Year\n",
            "0         ACRU  2025      7            83.0    2025-07\n",
            "1         ACRU  2025      8           164.6    2025-08\n",
            "2         ACRU  2025      9           223.8    2025-09\n",
            "3         ACRU  2025     10            72.0    2025-10\n",
            "4         ACRU  2025     11            53.6    2025-11\n",
            "ข้อมูลสรุปของ monthly_rainfall_summary:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2451 entries, 0 to 2450\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   station_code    2451 non-null   object \n",
            " 1   year            2451 non-null   int32  \n",
            " 2   month           2451 non-null   int32  \n",
            " 3   rainfall_value  2451 non-null   float64\n",
            " 4   Month-Year      2451 non-null   object \n",
            "dtypes: float64(1), int32(2), object(2)\n",
            "memory usage: 76.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23495be"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #4 of the subtask, I will use `pivot_table` to transform the `monthly_rainfall_summary` DataFrame, setting 'station_code' as the index, 'Month-Year' as columns, and 'rainfall_value' as the values. This will reshape the data into the desired output format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d6a71ed",
        "outputId": "65cbe3c3-9e73-4102-f24d-40530cf1591b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 4. ใช้ฟังก์ชัน `pivot_table` เพื่อเปลี่ยนรูปแบบ DataFrame\n",
        "# ให้มี 'station_code' เป็น index และ 'Month-Year' เป็นคอลัมน์ โดยมีค่าภายในเป็นผลรวมปริมาณน้ำฝน ('rainfall_value').\n",
        "pivoted_rainfall_df = monthly_rainfall_summary.pivot_table(\n",
        "    index='station_code',\n",
        "    columns='Month-Year',\n",
        "    values='rainfall_value',\n",
        "    fill_value=0 # Fill any potential missing combinations with 0 rainfall\n",
        ")\n",
        "\n",
        "print(\"Pivoted the monthly rainfall summary to station_code as index and Month-Year as columns.\")\n",
        "print(\"5 แถวแรกของ pivoted_rainfall_df:\")\n",
        "print(pivoted_rainfall_df.head())\n",
        "print(\"ข้อมูลสรุปของ pivoted_rainfall_df:\")\n",
        "pivoted_rainfall_df.info()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pivoted the monthly rainfall summary to station_code as index and Month-Year as columns.\n",
            "5 แถวแรกของ pivoted_rainfall_df:\n",
            "Month-Year    2025-07  2025-08  2025-09  2025-10  2025-11  2025-12  2026-01\n",
            "station_code                                                               \n",
            "ACRU             83.0    164.6    223.8     72.0     53.6      0.2      0.0\n",
            "BBHN            390.6    537.4    619.0    197.6    228.2      6.2      0.0\n",
            "BBUA            135.6     57.2      8.4     79.0      4.4      0.0      0.0\n",
            "BCNG            240.0    156.2    172.8     31.2     11.8     23.0      0.0\n",
            "BDAR             37.0    105.6     98.8     36.0     84.6      4.2      0.0\n",
            "ข้อมูลสรุปของ pivoted_rainfall_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 362 entries, ACRU to YOM012\n",
            "Data columns (total 7 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   2025-07  362 non-null    float64\n",
            " 1   2025-08  362 non-null    float64\n",
            " 2   2025-09  362 non-null    float64\n",
            " 3   2025-10  362 non-null    float64\n",
            " 4   2025-11  362 non-null    float64\n",
            " 5   2025-12  362 non-null    float64\n",
            " 6   2026-01  362 non-null    float64\n",
            "dtypes: float64(7)\n",
            "memory usage: 22.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6506dbab"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #5 of the subtask, I will sort the columns of the `pivoted_rainfall_df` chronologically to ensure they are in the correct 'Month-Year' order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a630694",
        "outputId": "f793b0ec-3a06-4196-ea71-899c1415f14e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 5. จัดเรียงคอลัมน์ 'Month-Year' ตามลำดับเวลา\n",
        "# เนื่องจากชื่อคอลัมน์เป็นรูปแบบ 'YYYY-MM' (หรือ 'YYYY-MM-DD' ถ้ามีวัน) การเรียงตามตัวอักษรจะให้ผลลัพธ์ที่ถูกต้องอยู่แล้ว\n",
        "# แต่เพื่อความแน่ใจ จะแปลงเป็น datetime object ก่อนเรียงแล้วค่อยแปลงกลับเป็น string\n",
        "\n",
        "# Get current columns (excluding the index name if it exists as a column level)\n",
        "current_columns = pivoted_rainfall_df.columns.tolist()\n",
        "\n",
        "# Convert column names to datetime objects for accurate chronological sorting\n",
        "sorted_columns = pd.to_datetime(current_columns, format='%Y-%m').strftime('%Y-%m').tolist()\n",
        "\n",
        "# Reindex the DataFrame with the sorted columns\n",
        "pivoted_rainfall_df = pivoted_rainfall_df[sorted_columns]\n",
        "\n",
        "print(\"Sorted 'Month-Year' columns chronologically.\")\n",
        "print(\"5 แถวแรกของ pivoted_rainfall_df หลังจัดเรียงคอลัมน์:\")\n",
        "print(pivoted_rainfall_df.head())\n",
        "print(\"ข้อมูลสรุปของ pivoted_rainfall_df หลังจัดเรียงคอลัมน์:\")\n",
        "pivoted_rainfall_df.info()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted 'Month-Year' columns chronologically.\n",
            "5 แถวแรกของ pivoted_rainfall_df หลังจัดเรียงคอลัมน์:\n",
            "Month-Year    2025-07  2025-08  2025-09  2025-10  2025-11  2025-12  2026-01\n",
            "station_code                                                               \n",
            "ACRU             83.0    164.6    223.8     72.0     53.6      0.2      0.0\n",
            "BBHN            390.6    537.4    619.0    197.6    228.2      6.2      0.0\n",
            "BBUA            135.6     57.2      8.4     79.0      4.4      0.0      0.0\n",
            "BCNG            240.0    156.2    172.8     31.2     11.8     23.0      0.0\n",
            "BDAR             37.0    105.6     98.8     36.0     84.6      4.2      0.0\n",
            "ข้อมูลสรุปของ pivoted_rainfall_df หลังจัดเรียงคอลัมน์:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 362 entries, ACRU to YOM012\n",
            "Data columns (total 7 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   2025-07  362 non-null    float64\n",
            " 1   2025-08  362 non-null    float64\n",
            " 2   2025-09  362 non-null    float64\n",
            " 3   2025-10  362 non-null    float64\n",
            " 4   2025-11  362 non-null    float64\n",
            " 5   2025-12  362 non-null    float64\n",
            " 6   2026-01  362 non-null    float64\n",
            "dtypes: float64(7)\n",
            "memory usage: 22.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf4345c5"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #6 of the subtask, I will remove the name of the column index ('Month-Year') from `pivoted_rainfall_df` to make the column headers cleaner and ensure they are just the month-year strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8837ed89",
        "outputId": "cbc066f9-c1a3-47de-ac9c-618be74b1297"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 6. เปลี่ยนชื่อคอลัมน์ที่ได้จากการ pivot ให้เหมาะสม โดยลบชื่อระดับคอลัมน์ออกไป\n",
        "pivoted_rainfall_df.columns.name = None\n",
        "\n",
        "print(\"Removed column index name.\")\n",
        "print(\"5 แถวแรกของ pivoted_rainfall_df หลังลบชื่อคอลัมน์อินเด็กซ์:\")\n",
        "print(pivoted_rainfall_df.head())\n",
        "print(\"ข้อมูลสรุปของ pivoted_rainfall_df หลังลบชื่อคอลัมน์อินเด็กซ์:\")\n",
        "pivoted_rainfall_df.info()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed column index name.\n",
            "5 แถวแรกของ pivoted_rainfall_df หลังลบชื่อคอลัมน์อินเด็กซ์:\n",
            "              2025-07  2025-08  2025-09  2025-10  2025-11  2025-12  2026-01\n",
            "station_code                                                               \n",
            "ACRU             83.0    164.6    223.8     72.0     53.6      0.2      0.0\n",
            "BBHN            390.6    537.4    619.0    197.6    228.2      6.2      0.0\n",
            "BBUA            135.6     57.2      8.4     79.0      4.4      0.0      0.0\n",
            "BCNG            240.0    156.2    172.8     31.2     11.8     23.0      0.0\n",
            "BDAR             37.0    105.6     98.8     36.0     84.6      4.2      0.0\n",
            "ข้อมูลสรุปของ pivoted_rainfall_df หลังลบชื่อคอลัมน์อินเด็กซ์:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 362 entries, ACRU to YOM012\n",
            "Data columns (total 7 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   2025-07  362 non-null    float64\n",
            " 1   2025-08  362 non-null    float64\n",
            " 2   2025-09  362 non-null    float64\n",
            " 3   2025-10  362 non-null    float64\n",
            " 4   2025-11  362 non-null    float64\n",
            " 5   2025-12  362 non-null    float64\n",
            " 6   2026-01  362 non-null    float64\n",
            "dtypes: float64(7)\n",
            "memory usage: 22.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d1ed18"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #7 of the subtask, I will reset the index of the `pivoted_rainfall_df` to convert 'station_code' from an index to a regular column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ea94422",
        "outputId": "7e0117f8-1ebf-4cf0-830e-2ef72d84976a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 7. รีเซ็ต index ของ DataFrame เพื่อให้ 'station_code' เป็นคอลัมน์ปกติ.\n",
        "pivoted_rainfall_df = pivoted_rainfall_df.reset_index()\n",
        "\n",
        "print(\"Reset DataFrame index.\")\n",
        "print(\"5 แถวแรกของ pivoted_rainfall_df หลังรีเซ็ตอินเด็กซ์:\")\n",
        "print(pivoted_rainfall_df.head())\n",
        "print(\"ข้อมูลสรุปของ pivoted_rainfall_df หลังรีเซ็ตอินเด็กซ์:\")\n",
        "pivoted_rainfall_df.info()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reset DataFrame index.\n",
            "5 แถวแรกของ pivoted_rainfall_df หลังรีเซ็ตอินเด็กซ์:\n",
            "  station_code  2025-07  2025-08  2025-09  2025-10  2025-11  2025-12  2026-01\n",
            "0         ACRU     83.0    164.6    223.8     72.0     53.6      0.2      0.0\n",
            "1         BBHN    390.6    537.4    619.0    197.6    228.2      6.2      0.0\n",
            "2         BBUA    135.6     57.2      8.4     79.0      4.4      0.0      0.0\n",
            "3         BCNG    240.0    156.2    172.8     31.2     11.8     23.0      0.0\n",
            "4         BDAR     37.0    105.6     98.8     36.0     84.6      4.2      0.0\n",
            "ข้อมูลสรุปของ pivoted_rainfall_df หลังรีเซ็ตอินเด็กซ์:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 362 entries, 0 to 361\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   station_code  362 non-null    object \n",
            " 1   2025-07       362 non-null    float64\n",
            " 2   2025-08       362 non-null    float64\n",
            " 3   2025-09       362 non-null    float64\n",
            " 4   2025-10       362 non-null    float64\n",
            " 5   2025-11       362 non-null    float64\n",
            " 6   2025-12       362 non-null    float64\n",
            " 7   2026-01       362 non-null    float64\n",
            "dtypes: float64(7), object(1)\n",
            "memory usage: 22.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d01446e"
      },
      "source": [
        "## Save Monthly Rainfall Output\n",
        "\n",
        "### Subtask:\n",
        "บันทึกผลลัพธ์ข้อมูลปริมาณน้ำฝนรายเดือนที่ได้จากการคำนวณและจัดรูปแบบแล้ว ลงในไฟล์ CSV ใหม่ชื่อ 'Y_montly_rain_hii_operation.csv' พร้อมเตรียมลิงก์ Google Drive ของไฟล์นี้ที่เปิดเป็นสาธารณะ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6fa6fba"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will save the `pivoted_rainfall_df` to the specified CSV file path without including the index. Then, I will verify the file's existence. Following that, I will attempt to generate a public Google Drive link for the saved file using PyDrive, handling the necessary authentication steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "00b4f113",
        "outputId": "3b02f8c4-9865-4231-8200-7593899ecbf8"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "# 1. บันทึก DataFrame `pivoted_rainfall_df` เป็นไฟล์ CSV\n",
        "# output_monthly_rainfall_file is defined as '/content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
        "pivoted_rainfall_df.to_csv(output_monthly_rainfall_file, index=False)\n",
        "\n",
        "print(f\"ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ได้ถูกบันทึกที่: {output_monthly_rainfall_file}\")\n",
        "\n",
        "# 2. ตรวจสอบว่าไฟล์ถูกบันทึกสำเร็จ\n",
        "if os.path.exists(output_monthly_rainfall_file):\n",
        "    print(f\"ยืนยัน: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' บันทึกสำเร็จ.\")\n",
        "else:\n",
        "    print(f\"ข้อผิดพลาด: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ไม่พบบนไดรฟ์.\")\n",
        "\n",
        "# 3. สร้างลิงก์ Google Drive สาธารณะสำหรับไฟล์\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = auth.get_user_credentials(auth.DEFAULT_CREDENTIALS_FILE)\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the full path components to search for the file\n",
        "drive_path_components = output_monthly_rainfall_file.replace('/content/drive/MyDrive/', '').split('/')\n",
        "file_name_to_share = os.path.basename(output_monthly_rainfall_file)\n",
        "\n",
        "# Function to find file/folder ID by name and parent ID\n",
        "def get_drive_id(name, parent_id=None, is_folder=False):\n",
        "    query = f\"trashed=false and title = '{name}'\"\n",
        "    if parent_id:\n",
        "        query += f\" and '{parent_id}' in parents\"\n",
        "    if is_folder:\n",
        "        query += f\" and mimeType = 'application/vnd.google-apps.folder'\"\n",
        "    else:\n",
        "        query += f\" and mimeType != 'application/vnd.google-apps.folder'\"\n",
        "    file_list = drive.ListFile({'q': query}).GetList()\n",
        "    return file_list[0]['id'] if file_list else None\n",
        "\n",
        "# Traverse folders to find the parent folder ID where the file is stored\n",
        "current_parent_id = 'root' # Start from My Drive\n",
        "for component in drive_path_components[:-1]: # Iterate through folders\n",
        "    current_parent_id = get_drive_id(component, current_parent_id, is_folder=True)\n",
        "    if not current_parent_id:\n",
        "        print(f\"Error: Parent folder '{component}' not found. Cannot generate public link.\")\n",
        "        file_id = None\n",
        "        break\n",
        "\n",
        "if current_parent_id:\n",
        "    file_id = get_drive_id(file_name_to_share, current_parent_id)\n",
        "\n",
        "if file_id:\n",
        "    # Set permissions to anyone with the link can read\n",
        "    file_obj = drive.CreateFile({'id': file_id})\n",
        "    # Update permissions to 'reader' for 'anyone'\n",
        "    permission = file_obj.InsertPermission({\n",
        "        'type': 'anyone',\n",
        "        'value': 'anyone',\n",
        "        'role': 'reader'\n",
        "    })\n",
        "    file_obj.Upload()\n",
        "    public_link = file_obj['alternateLink']\n",
        "    print(f\"\\n4. ลิงก์ Google Drive สาธารณะสำหรับไฟล์ '{file_name_to_share}': {public_link}\")\n",
        "else:\n",
        "    print(\"\\nไม่สามารถสร้างลิงก์สาธารณะได้: ไม่พบไฟล์บน Google Drive หรือเกิดข้อผิดพลาดในการเข้าถึง.\")\n",
        "    print(\"โปรดตรวจสอบพาธของไฟล์และสิทธิ์การเข้าถึงใน Google Drive หรือสร้างลิงก์ด้วยตนเอง.\")\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pydrive'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4077198998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydrive'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ed2726"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `pydrive` library was not installed. I will add a `!pip install pydrive` command to install the missing dependency, then retry the code to save the CSV and generate a Google Drive link.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca04f696",
        "outputId": "380a46b1-081b-454d-9d47-81116274949a"
      },
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/987.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.188.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.29.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.27.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2026.1.4)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=4d80b97160a86329582c8edac63be1f42c639d5ea2026f65b6d3a1ea95aef7af\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b39fd89"
      },
      "source": [
        "**Reasoning**:\n",
        "The `pydrive` library has been successfully installed. Now, I will re-execute the code to save the `pivoted_rainfall_df` to a CSV file, verify its existence, and then proceed to generate and display a public Google Drive link for the saved file using the newly installed `pydrive` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "30072e45",
        "outputId": "ef964da9-5b9f-465a-8247-ac1214e37a7d"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "# 1. บันทึก DataFrame `pivoted_rainfall_df` เป็นไฟล์ CSV\n",
        "# output_monthly_rainfall_file is defined as '/content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
        "pivoted_rainfall_df.to_csv(output_monthly_rainfall_file, index=False)\n",
        "\n",
        "print(f\"ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ได้ถูกบันทึกที่: {output_monthly_rainfall_file}\")\n",
        "\n",
        "# 2. ตรวจสอบว่าไฟล์ถูกบันทึกสำเร็จ\n",
        "if os.path.exists(output_monthly_rainfall_file):\n",
        "    print(f\"ยืนยัน: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' บันทึกสำเร็จ.\")\n",
        "else:\n",
        "    print(f\"ข้อผิดพลาด: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ไม่พบบนไดรฟ์.\")\n",
        "\n",
        "# 3. สร้างลิงก์ Google Drive สาธารณะสำหรับไฟล์\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = auth.get_user_credentials(auth.DEFAULT_CREDENTIALS_FILE)\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the full path components to search for the file\n",
        "drive_path_components = output_monthly_rainfall_file.replace('/content/drive/MyDrive/', '').split('/')\n",
        "file_name_to_share = os.path.basename(output_monthly_rainfall_file)\n",
        "\n",
        "# Function to find file/folder ID by name and parent ID\n",
        "def get_drive_id(name, parent_id=None, is_folder=False):\n",
        "    query = f\"trashed=false and title = '{name}'\"\n",
        "    if parent_id:\n",
        "        query += f\" and '{parent_id}' in parents\"\n",
        "    if is_folder:\n",
        "        query += f\" and mimeType = 'application/vnd.google-apps.folder'\"\n",
        "    else:\n",
        "        query += f\" and mimeType != 'application/vnd.google-apps.folder'\"\n",
        "    file_list = drive.ListFile({'q': query}).GetList()\n",
        "    return file_list[0]['id'] if file_list else None\n",
        "\n",
        "# Traverse folders to find the parent folder ID where the file is stored\n",
        "current_parent_id = 'root' # Start from My Drive\n",
        "for component in drive_path_components[:-1]: # Iterate through folders\n",
        "    current_parent_id = get_drive_id(component, current_parent_id, is_folder=True)\n",
        "    if not current_parent_id:\n",
        "        print(f\"Error: Parent folder '{component}' not found. Cannot generate public link.\")\n",
        "        file_id = None\n",
        "        break\n",
        "\n",
        "if current_parent_id:\n",
        "    file_id = get_drive_id(file_name_to_share, current_parent_id)\n",
        "\n",
        "if file_id:\n",
        "    # Set permissions to anyone with the link can read\n",
        "    file_obj = drive.CreateFile({'id': file_id})\n",
        "    # Update permissions to 'reader' for 'anyone'\n",
        "    permission = file_obj.InsertPermission({\n",
        "        'type': 'anyone',\n",
        "        'value': 'anyone',\n",
        "        'role': 'reader'\n",
        "    })\n",
        "    file_obj.Upload()\n",
        "    public_link = file_obj['alternateLink']\n",
        "    print(f\"\\n4. ลิงก์ Google Drive สาธารณะสำหรับไฟล์ '{file_name_to_share}': {public_link}\")\n",
        "else:\n",
        "    print(\"\\nไม่สามารถสร้างลิงก์สาธารณะได้: ไม่พบไฟล์บน Google Drive หรือเกิดข้อผิดพลาดในการเข้าถึง.\")\n",
        "    print(\"โปรดตรวจสอบพาธของไฟล์และสิทธิ์การเข้าถึงใน Google Drive หรือสร้างลิงก์ด้วยตนเอง.\")\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ไฟล์ 'Y_montly_rain_hii_operation.csv' ได้ถูกบันทึกที่: /content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n",
            "ยืนยัน: ไฟล์ 'Y_montly_rain_hii_operation.csv' บันทึกสำเร็จ.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'google.colab.auth' has no attribute 'get_user_credentials'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4077198998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_user_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_CREDENTIALS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdrive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'google.colab.auth' has no attribute 'get_user_credentials'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "541a90e7"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `AttributeError: module 'google.colab.auth' has no attribute 'get_user_credentials'` indicates that the method used to get credentials for `PyDrive` is deprecated or incorrect. I will replace it with the standard `gauth.Authenticate()` method, which correctly handles authentication within Google Colab for PyDrive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "b666a687",
        "outputId": "9c5fa5cf-8d21-4d11-aec1-765781c439e2"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "# 1. บันทึก DataFrame `pivoted_rainfall_df` เป็นไฟล์ CSV\n",
        "# output_monthly_rainfall_file is defined as '/content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
        "pivoted_rainfall_df.to_csv(output_monthly_rainfall_file, index=False)\n",
        "\n",
        "print(f\"ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ได้ถูกบันทึกที่: {output_monthly_rainfall_file}\")\n",
        "\n",
        "# 2. ตรวจสอบว่าไฟล์ถูกบันทึกสำเร็จ\n",
        "if os.path.exists(output_monthly_rainfall_file):\n",
        "    print(f\"ยืนยัน: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' บันทึกสำเร็จ.\")\n",
        "else:\n",
        "    print(f\"ข้อผิดพลาด: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ไม่พบบนไดรฟ์.\")\n",
        "\n",
        "# 3. สร้างลิงก์ Google Drive สาธารณะสำหรับไฟล์\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.Authenticate() # Corrected authentication method for PyDrive\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the full path components to search for the file\n",
        "drive_path_components = output_monthly_rainfall_file.replace('/content/drive/MyDrive/', '').split('/')\n",
        "file_name_to_share = os.path.basename(output_monthly_rainfall_file)\n",
        "\n",
        "# Function to find file/folder ID by name and parent ID\n",
        "def get_drive_id(name, parent_id=None, is_folder=False):\n",
        "    query = f\"trashed=false and title = '{name}'\"\n",
        "    if parent_id:\n",
        "        query += f\" and '{parent_id}' in parents\"\n",
        "    if is_folder:\n",
        "        query += f\" and mimeType = 'application/vnd.google-apps.folder'\"\n",
        "    else:\n",
        "        query += f\" and mimeType != 'application/vnd.google-apps.folder'\"\n",
        "    file_list = drive.ListFile({'q': query}).GetList()\n",
        "    return file_list[0]['id'] if file_list else None\n",
        "\n",
        "# Traverse folders to find the parent folder ID where the file is stored\n",
        "current_parent_id = 'root' # Start from My Drive\n",
        "for component in drive_path_components[:-1]: # Iterate through folders\n",
        "    current_parent_id = get_drive_id(component, current_parent_id, is_folder=True)\n",
        "    if not current_parent_id:\n",
        "        print(f\"Error: Parent folder '{component}' not found. Cannot generate public link.\")\n",
        "        file_id = None\n",
        "        break\n",
        "\n",
        "if current_parent_id:\n",
        "    file_id = get_drive_id(file_name_to_share, current_parent_id)\n",
        "\n",
        "if file_id:\n",
        "    # Set permissions to anyone with the link can read\n",
        "    file_obj = drive.CreateFile({'id': file_id})\n",
        "    # Update permissions to 'reader' for 'anyone'\n",
        "    permission = file_obj.InsertPermission({\n",
        "        'type': 'anyone',\n",
        "        'value': 'anyone',\n",
        "        'role': 'reader'\n",
        "    })\n",
        "    file_obj.Upload()\n",
        "    public_link = file_obj['alternateLink']\n",
        "    print(f\"\\n4. ลิงก์ Google Drive สาธารณะสำหรับไฟล์ '{file_name_to_share}': {public_link}\")\n",
        "else:\n",
        "    print(\"\\nไม่สามารถสร้างลิงก์สาธารณะได้: ไม่พบไฟล์บน Google Drive หรือเกิดข้อผิดพลาดในการเข้าถึง.\")\n",
        "    print(\"โปรดตรวจสอบพาธของไฟล์และสิทธิ์การเข้าถึงใน Google Drive หรือสร้างลิงก์ด้วยตนเอง.\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ไฟล์ 'Y_montly_rain_hii_operation.csv' ได้ถูกบันทึกที่: /content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n",
            "ยืนยัน: ไฟล์ 'Y_montly_rain_hii_operation.csv' บันทึกสำเร็จ.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "GoogleAuth.Authenticate() missing 1 required positional argument: 'code'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-470242796.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAuthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Corrected authentication method for PyDrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdrive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: GoogleAuth.Authenticate() missing 1 required positional argument: 'code'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57327376"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `TypeError: GoogleAuth.Authenticate() missing 1 required positional argument: 'code'` indicates that the `gauth.Authenticate()` method is incorrect for Google Colab's authentication flow. I will replace it with `gauth.LocalWebserverAuth()` which is a standard PyDrive method that typically integrates well with `google.colab.auth` after `auth.authenticate_user()` has been called.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "db1b1abd",
        "outputId": "2164592c-9162-4be0-ab54-79434c25ecf4"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "# 1. บันทึก DataFrame `pivoted_rainfall_df` เป็นไฟล์ CSV\n",
        "# output_monthly_rainfall_file is defined as '/content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
        "pivoted_rainfall_df.to_csv(output_monthly_rainfall_file, index=False)\n",
        "\n",
        "print(f\"ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ได้ถูกบันทึกที่: {output_monthly_rainfall_file}\")\n",
        "\n",
        "# 2. ตรวจสอบว่าไฟล์ถูกบันทึกสำเร็จ\n",
        "if os.path.exists(output_monthly_rainfall_file):\n",
        "    print(f\"ยืนยัน: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' บันทึกสำเร็จ.\")\n",
        "else:\n",
        "    print(f\"ข้อผิดพลาด: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ไม่พบบนไดรฟ์.\")\n",
        "\n",
        "# 3. สร้างลิงก์ Google Drive สาธารณะสำหรับไฟล์\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.LocalWebserverAuth() # Corrected authentication method for PyDrive\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the full path components to search for the file\n",
        "drive_path_components = output_monthly_rainfall_file.replace('/content/drive/MyDrive/', '').split('/')\n",
        "file_name_to_share = os.path.basename(output_monthly_rainfall_file)\n",
        "\n",
        "# Function to find file/folder ID by name and parent ID\n",
        "def get_drive_id(name, parent_id=None, is_folder=False):\n",
        "    query = f\"trashed=false and title = '{name}'\"\n",
        "    if parent_id:\n",
        "        query += f\" and '{parent_id}' in parents\"\n",
        "    if is_folder:\n",
        "        query += f\" and mimeType = 'application/vnd.google-apps.folder'\"\n",
        "    else:\n",
        "        query += f\" and mimeType != 'application/vnd.google-apps.folder'\"\n",
        "    file_list = drive.ListFile({'q': query}).GetList()\n",
        "    return file_list[0]['id'] if file_list else None\n",
        "\n",
        "# Traverse folders to find the parent folder ID where the file is stored\n",
        "current_parent_id = 'root' # Start from My Drive\n",
        "for component in drive_path_components[:-1]: # Iterate through folders\n",
        "    current_parent_id = get_drive_id(component, current_parent_id, is_folder=True)\n",
        "    if not current_parent_id:\n",
        "        print(f\"Error: Parent folder '{component}' not found. Cannot generate public link.\")\n",
        "        file_id = None\n",
        "        break\n",
        "\n",
        "if current_parent_id:\n",
        "    file_id = get_drive_id(file_name_to_share, current_parent_id)\n",
        "\n",
        "if file_id:\n",
        "    # Set permissions to anyone with the link can read\n",
        "    file_obj = drive.CreateFile({'id': file_id})\n",
        "    # Update permissions to 'reader' for 'anyone'\n",
        "    permission = file_obj.InsertPermission({\n",
        "        'type': 'anyone',\n",
        "        'value': 'anyone',\n",
        "        'role': 'reader'\n",
        "    })\n",
        "    file_obj.Upload()\n",
        "    public_link = file_obj['alternateLink']\n",
        "    print(f\"\\n4. ลิงก์ Google Drive สาธารณะสำหรับไฟล์ '{file_name_to_share}': {public_link}\")\n",
        "else:\n",
        "    print(\"\\nไม่สามารถสร้างลิงก์สาธารณะได้: ไม่พบไฟล์บน Google Drive หรือเกิดข้อผิดพลาดในการเข้าถึง.\")\n",
        "    print(\"โปรดตรวจสอบพาธของไฟล์และสิทธิ์การเข้าถึงใน Google Drive หรือสร้างลิงก์ด้วยตนเอง.\")\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ไฟล์ 'Y_montly_rain_hii_operation.csv' ได้ถูกบันทึกที่: /content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n",
            "ยืนยัน: ไฟล์ 'Y_montly_rain_hii_operation.csv' บันทึกสำเร็จ.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidConfigError",
          "evalue": "Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36m_loadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_secrets.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidClientSecretsError\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfigFile\u001b[0;34m(self, client_config_file)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m       \u001b[0mclient_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidClientSecretsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36mloadfile\u001b[0;34m(filename, cache)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_loadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36m_loadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         raise InvalidClientSecretsError('Error opening file', exc.filename,\n\u001b[0m\u001b[1;32m    125\u001b[0m                                         exc.strerror, exc.errno)\n",
            "\u001b[0;31mInvalidClientSecretsError\u001b[0m: ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidConfigError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1571367397.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocalWebserverAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Corrected authentication method for PyDrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdrive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadCredentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetFlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mGetFlow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     if not all(config in self.client_config \\\n\u001b[1;32m    442\u001b[0m                for config in self.CLIENT_CONFIGS_LIST):\n\u001b[0;32m--> 443\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     constructor_kwargs = {\n\u001b[1;32m    445\u001b[0m         \u001b[0;34m'redirect_uri'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'redirect_uri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfig\u001b[0;34m(self, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mInvalidConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please specify client config backend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfigFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'settings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfigSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfigFile\u001b[0;34m(self, client_config_file)\u001b[0m\n\u001b[1;32m    386\u001b[0m       \u001b[0mclient_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidClientSecretsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mInvalidConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid client secrets file %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     if not client_type in (clientsecrets.TYPE_WEB,\n\u001b[1;32m    390\u001b[0m                            clientsecrets.TYPE_INSTALLED):\n",
            "\u001b[0;31mInvalidConfigError\u001b[0m: Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgYkAqewsJmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8172d73b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed to generate a public Google Drive link due to persistent issues with programmatic authentication and file sharing via `PyDrive`, `gdown`, and direct `googleapiclient` methods within the Colab environment. Given these consistent failures, the most robust solution is to clearly instruct the user on how to manually generate the public link for the saved file. This ensures the output CSV is saved and the user can fulfill the task requirement for a public link, even if automation is not feasible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2aad5ed",
        "outputId": "583e4828-b87e-4716-95ff-afc9d33ba118"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. บันทึก DataFrame `final_output_df` เป็นไฟล์ CSV\n",
        "# output_monthly_rainfall_file is defined as '/content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
        "final_output_df.to_csv(output_monthly_rainfall_file, index=False)\n",
        "\n",
        "print(f\"ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ได้ถูกบันทึกที่: {output_monthly_rainfall_file}\")\n",
        "\n",
        "# 2. ตรวจสอบว่าไฟล์ถูกบันทึกสำเร็จ\n",
        "if os.path.exists(output_monthly_rainfall_file):\n",
        "    print(f\"ยืนยัน: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' บันทึกสำเร็จ.\")\n",
        "else:\n",
        "    print(f\"ข้อผิดพลาด: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ไม่พบบนไดรฟ์.\")\n",
        "\n",
        "# 3. สร้างลิงก์ Google Drive สาธารณะสำหรับไฟล์ (ให้คำแนะนำ)\n",
        "print(\"\\n**คำแนะนำสำหรับสร้างลิงก์ Google Drive สาธารณะ:**\")\n",
        "print(\"เนื่องจากปัญหาทางเทคนิคในการสร้างลิงก์สาธารณะด้วยโค้ดโดยตรงใน Google Colab\")\n",
        "print(\"โปรดทำตามขั้นตอนต่อไปนี้เพื่อสร้างลิงก์ด้วยตนเอง:\")\n",
        "print(\"1. ไปที่ Google Drive ของคุณ: https://drive.google.com/\")\n",
        "print(f\"2. ค้นหาไฟล์ชื่อ: '{os.path.basename(output_monthly_rainfall_file)}' ที่พาธ: '{output_monthly_rainfall_file.replace('/content/drive', '')}'\")\n",
        "print(\"3. คลิกขวาที่ไฟล์ > 'รับลิงก์' (Get link)\")\n",
        "print(\"4. เปลี่ยน 'จำกัด' (Restricted) เป็น 'ทุกคนที่มีลิงก์' (Anyone with the link)\")\n",
        "print(\"5. คัดลอกลิงก์ที่ได้มาและนำไปใช้งาน\")\n",
        "\n",
        "# Provide the expected location of the file in Google Drive for easy access\n",
        "file_location_in_drive = output_monthly_rainfall_file.replace('/content/drive/MyDrive/', 'My Drive/')\n",
        "print(f\"\\nไฟล์ของคุณจะอยู่ที่: {file_location_in_drive}\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ไฟล์ 'Y_montly_rain_hii_operation.csv' ได้ถูกบันทึกที่: /content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n",
            "ยืนยัน: ไฟล์ 'Y_montly_rain_hii_operation.csv' บันทึกสำเร็จ.\n",
            "\n",
            "**คำแนะนำสำหรับสร้างลิงก์ Google Drive สาธารณะ:**\n",
            "เนื่องจากปัญหาทางเทคนิคในการสร้างลิงก์สาธารณะด้วยโค้ดโดยตรงใน Google Colab\n",
            "โปรดทำตามขั้นตอนต่อไปนี้เพื่อสร้างลิงก์ด้วยตนเอง:\n",
            "1. ไปที่ Google Drive ของคุณ: https://drive.google.com/\n",
            "2. ค้นหาไฟล์ชื่อ: 'Y_montly_rain_hii_operation.csv' ที่พาธ: '/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
            "3. คลิกขวาที่ไฟล์ > 'รับลิงก์' (Get link)\n",
            "4. เปลี่ยน 'จำกัด' (Restricted) เป็น 'ทุกคนที่มีลิงก์' (Anyone with the link)\n",
            "5. คัดลอกลิงก์ที่ได้มาและนำไปใช้งาน\n",
            "\n",
            "ไฟล์ของคุณจะอยู่ที่: My Drive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d794b33"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "จัดเตรียมไฟล์ Colab Notebook (.ipynb) ที่มีโค้ดและคำอธิบายครบถ้วนตามที่ร้องขอ และแจ้งลิงก์ Google Drive ของไฟล์ผลลัพธ์ข้อมูลรายเดือน 'Y_montly_rain_hii_operation.csv' ที่เปิดเป็นสาธารณะ พร้อมบันทึกการใช้ AI (prompt, ผลลัพธ์, การปรับแก้, เหตุผล) เพื่อส่งมอบงาน\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8e68af0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Loading and Preparation**:\n",
        "    *   Successfully loaded three main datasets: `Stations in the model_HII.csv` (385 unique stations), `hii_daily_rain.csv` (historical data), and operational daily rainfall data (176,882 records from 1,514 unique stations across multiple files).\n",
        "    *   Date/time and rainfall value columns were consistently identified (`rainfall_datetime`, `rainfall_value` for operational; `measure_datetime`, `data` for historical) and converted to appropriate data types.\n",
        "*   **Station Filtering**: The operational daily rainfall data was filtered to include only the 384 stations defined in `Stations in the model_HII.csv`, reducing the initial 1,514 unique stations.\n",
        "*   **Missing Data Identification**: For the 384 filtered stations, daily missing records were quantified. The analysis revealed a maximum missing data percentage of 66.85% for a single station, with an average of 2.52% across all stations.\n",
        "*   **Missing Data Handling**:\n",
        "    *   22 stations with 20% or more missing daily rainfall records were removed, leaving 362 unique stations for further analysis.\n",
        "    *   Missing daily rainfall values (identified as 485 records after initial merging) for the remaining 362 stations were imputed using a hierarchical median approach: first, station-specific 'Day of Year' median from historical data; then, overall 'Day of Year' median; and finally, 0 for any remaining missing values. This ensured a complete dataset with 0 remaining missing values in the rainfall column.\n",
        "*   **Rationale for Imputation**: Median was chosen over mean for imputation due to the skewed nature of rainfall data (many zero values, occasional high outliers from heavy rain), as median is more robust to outliers and better reflects the typical rainfall amount.\n",
        "*   **Monthly Rainfall Calculation**: Monthly rainfall totals were successfully calculated for each of the 362 stations and formatted into a pivot table with `station_code` as rows and chronologically sorted 'YYYY-MM' as columns, with missing months filled with 0.\n",
        "*   **Output Saving**: The final monthly rainfall data was saved as `Y_montly_rain_hii_operation.csv` to the specified Google Drive path.\n",
        "*   **Google Drive Link Sharing**: Programmatic generation of a public Google Drive link faced technical challenges with `PyDrive` in the Colab environment. Therefore, clear manual instructions for generating a public link were provided to the user.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The robust data cleaning and imputation strategy (removing high-missing stations, using hierarchical median imputation) ensures high data quality for subsequent analysis, providing more reliable monthly rainfall totals.\n",
        "*   For future iterations, investigating a more direct or reliable method for programmatic Google Drive public link sharing within the Colab environment could streamline the output delivery process.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzIo3JtXtLnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0e94b2a"
      },
      "source": [
        "# Task\n",
        "The user has approved the current state, and the next step is to `Format Monthly Rainfall Output to Match Template`. This involves adjusting the DataFrame `monthly_rainfall_summary` to have the columns 'Node', 'rain', 'month', 'years' as specified in the plan. The existing `monthly_rainfall_summary` contains `station_code`, `year`, `month`, and `rainfall_value`. I will rename these columns to match the target template and then reorder them. After this, I will save this newly formatted DataFrame to the output CSV file, replacing the previously generated pivoted output, as the plan explicitly specifies the column names for the output.\n",
        "\n",
        "## Format Monthly Rainfall Output to Match Template\n",
        "\n",
        "### Subtask:\n",
        "ปรับรูปแบบ DataFrame `monthly_rainfall_summary` ให้มีคอลัมน์ 'Node', 'rain', 'month', 'years' ตามที่ผู้ใช้ต้องการ โดยเปลี่ยนชื่อคอลัมน์ที่มีอยู่ให้ตรงกันและจัดเรียงลำดับคอลัมน์ใหม่ จากนั้นจะแสดงผลข้อมูลเบื้องต้นของ DataFrame ที่ปรับรูปแบบแล้ว\n",
        "\n",
        "### Reasoning:\n",
        "As per the subtask, I need to modify the `monthly_rainfall_summary` DataFrame to match the target column names: 'Node', 'rain', 'month', 'years'. This involves renaming `station_code` to 'Node', `rainfall_value` to 'rain', and `year` to 'years'. I will then select and reorder these columns, dropping the intermediate 'Month-Year' column. This will create the final output format before saving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11f05666"
      },
      "source": [
        "## Format Monthly Rainfall Output to Match Template\n",
        "\n",
        "### Subtask:\n",
        "ปรับรูปแบบ DataFrame `monthly_rainfall_summary` ให้มีคอลัมน์ 'Node', 'rain', 'month', 'years' ตามที่ผู้ใช้ต้องการ โดยเปลี่ยนชื่อคอลัมน์ที่มีอยู่ให้ตรงกันและจัดเรียงลำดับคอลัมน์ใหม่ จากนั้นจะแสดงผลข้อมูลเบื้องต้นของ DataFrame ที่ปรับรูปแบบแล้ว\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d954c226"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will make a copy of the `monthly_rainfall_summary` DataFrame to avoid modifying the original. Then, I will rename the specified columns to match the required names: 'station_code' to 'Node', 'rainfall_value' to 'rain', and 'year' to 'years'. This aligns with instruction #1 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bc8f6b6",
        "outputId": "2752c5b3-3348-42a7-c59f-247ef50623ec"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. สร้าง DataFrame ใหม่โดยคัดลอก `monthly_rainfall_summary`\n",
        "# และเปลี่ยนชื่อคอลัมน์ดังนี้: 'station_code' เป็น 'Node', 'rainfall_value' เป็น 'rain', และ 'year' เป็น 'years'\n",
        "formatted_monthly_rainfall_df = monthly_rainfall_summary.rename(columns={\n",
        "    'station_code': 'Node',\n",
        "    'rainfall_value': 'rain',\n",
        "    'year': 'years'\n",
        "}).copy()\n",
        "\n",
        "print(\"Columns renamed in formatted_monthly_rainfall_df.\")\n",
        "print(\"5 แถวแรกของ formatted_monthly_rainfall_df หลังเปลี่ยนชื่อคอลัมน์:\")\n",
        "print(formatted_monthly_rainfall_df.head())"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns renamed in formatted_monthly_rainfall_df.\n",
            "5 แถวแรกของ formatted_monthly_rainfall_df หลังเปลี่ยนชื่อคอลัมน์:\n",
            "   Node  years  month   rain Month-Year\n",
            "0  ACRU   2025      7   83.0    2025-07\n",
            "1  ACRU   2025      8  164.6    2025-08\n",
            "2  ACRU   2025      9  223.8    2025-09\n",
            "3  ACRU   2025     10   72.0    2025-10\n",
            "4  ACRU   2025     11   53.6    2025-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c270aa8a"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #2 of the subtask, I will now reorder the columns in the `formatted_monthly_rainfall_df` to match the specified order: 'Node', 'rain', 'month', 'years'. The intermediate 'Month-Year' column is not part of the final output format and will be dropped along with any other unused columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2d92223",
        "outputId": "a2716fce-4a8e-4abb-e4ac-623ce89750e7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 2. จัดเรียงลำดับคอลัมน์ใน DataFrame ที่สร้างขึ้นใหม่ให้เป็น: 'Node', 'rain', 'month', 'years'\n",
        "# เลือกเฉพาะคอลัมน์ที่ต้องการและจัดเรียงลำดับ\n",
        "final_output_df = formatted_monthly_rainfall_df[['Node', 'rain', 'month', 'years']].copy()\n",
        "\n",
        "print(\"Columns reordered in final_output_df.\")\n",
        "print(\"5 แถวแรกของ final_output_df หลังจัดเรียงคอลัมน์:\")\n",
        "print(final_output_df.head())\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns reordered in final_output_df.\n",
            "5 แถวแรกของ final_output_df หลังจัดเรียงคอลัมน์:\n",
            "   Node   rain  month  years\n",
            "0  ACRU   83.0      7   2025\n",
            "1  ACRU  164.6      8   2025\n",
            "2  ACRU  223.8      9   2025\n",
            "3  ACRU   72.0     10   2025\n",
            "4  ACRU   53.6     11   2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0f961bc"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction #4 of the subtask, I will now display the summary information of the `final_output_df` using `info()` to verify data types and non-null counts, confirming the data is in the expected format after renaming and reordering columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ff9f42",
        "outputId": "f00f5b1c-6161-40ab-f824-52c58781e444"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 4. แสดงข้อมูลสรุปของ DataFrame ที่ถูกปรับรูปแบบแล้ว (เช่น `info()`) เพื่อตรวจสอบชนิดข้อมูลและ Non-Null Count\n",
        "print(\"\\nข้อมูลสรุปของ final_output_df:\")\n",
        "final_output_df.info()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ข้อมูลสรุปของ final_output_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2451 entries, 0 to 2450\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Node    2451 non-null   object \n",
            " 1   rain    2451 non-null   float64\n",
            " 2   month   2451 non-null   int32  \n",
            " 3   years   2451 non-null   int32  \n",
            "dtypes: float64(1), int32(2), object(1)\n",
            "memory usage: 57.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b892b69d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The DataFrame `monthly_rainfall_summary` was successfully transformed to meet the specified output format.\n",
        "*   Columns were renamed as follows: `station_code` to 'Node', `rainfall_value` to 'rain', and `year` to 'years'.\n",
        "*   The final DataFrame, `final_output_df`, contains the desired columns in the correct order: 'Node', 'rain', 'month', 'years'.\n",
        "*   The `final_output_df` contains 2451 non-null entries across all four columns, indicating data integrity.\n",
        "*   The data types for the final columns are `object` for 'Node', `float64` for 'rain', and `int32` for 'month' and 'years', which are appropriate for their respective contents.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The `final_output_df` is now correctly formatted and ready to be saved as the output CSV, replacing any previously generated files.\n",
        "*   The structured output facilitates further analysis or integration with other systems requiring this specific format.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfWlmXKkui9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d3f05e0"
      },
      "source": [
        "# Task\n",
        "The user has approved the current state. The next step is to filter the monthly rainfall data to include only the specific stations requested.\n",
        "\n",
        "## Filter Monthly Rainfall Data to Specific Nodes\n",
        "\n",
        "### Subtask:\n",
        "กรองข้อมูลปริมาณน้ำฝนรายเดือน (`final_output_df`) ให้เหลือเฉพาะสถานีที่ผู้ใช้ระบุเท่านั้น (ACRU, BBHN, BBUA, YOM012) เพื่อให้ผลลัพธ์สุดท้ายมีข้อมูลเฉพาะสถานีที่ต้องการ และแสดงผลข้อมูลเบื้องต้นของ DataFrame ที่กรองแล้ว\n",
        "\n",
        "### Reasoning:\n",
        "The task explicitly requests that the final output should only contain data for specific stations ('ACRU', 'BBHN', 'BBUA', 'YOM012'). While the previous steps processed and aggregated data for all valid stations, this step will now filter the `final_output_df` to include only these four specified stations. This ensures the output matches the required scope for the final delivery. I will then display the head of the filtered DataFrame to confirm the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7537f416"
      },
      "source": [
        "## Filter Monthly Rainfall Data to Specific Nodes\n",
        "\n",
        "### Subtask:\n",
        "กรองข้อมูลปริมาณน้ำฝนรายเดือน (`final_output_df`) ให้เหลือเฉพาะสถานีที่ผู้ใช้ระบุเท่านั้น (ACRU, BBHN, BBUA, YOM012) เพื่อให้ผลลัพธ์สุดท้ายมีข้อมูลเฉพาะสถานีที่ต้องการ และแสดงผลข้อมูลเบื้องต้นของ DataFrame ที่กรองแล้ว\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9ea1af"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a list of the desired station codes and then filter the `final_output_df` to include only these stations, storing the result in `filtered_final_output_df` as per instruction #1 and #2 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18b447e5",
        "outputId": "256e0527-42bf-426c-f730-aa7a4678c070"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. สร้างลิสต์ของชื่อสถานีที่ต้องการกรอง\n",
        "desired_stations = ['ACRU', 'BBHN', 'BBUA', 'YOM012']\n",
        "\n",
        "# กรอง final_output_df ด้วยสถานีที่ต้องการและไม่รวมปี 2026\n",
        "# เราจะกรองปี 2026 ออกก่อนสำหรับทุกสถานีที่ต้องการ\n",
        "base_filtered_df = final_output_df[\n",
        "    (final_output_df['Node'].isin(desired_stations)) &\n",
        "    (final_output_df['years'] != 2026)\n",
        "].copy()\n",
        "\n",
        "# เตรียมลิสต์เพื่อเก็บ DataFrame ที่ถูกกรองตามเงื่อนไขเฉพาะ\n",
        "final_filtered_list = []\n",
        "\n",
        "# กรองสำหรับสถานี ACRU และ BBHN (ไม่มีเงื่อนไขเดือนเฉพาะเจาะจง)\n",
        "acru_bbhn_data = base_filtered_df[base_filtered_df['Node'].isin(['ACRU', 'BBHN'])].copy()\n",
        "final_filtered_list.append(acru_bbhn_data)\n",
        "\n",
        "# กรองสำหรับสถานี BBUA (เดือน 7, 8, 9, 10 เท่านั้น)\n",
        "bbua_data = base_filtered_df[base_filtered_df['Node'] == 'BBUA'].copy()\n",
        "bbua_data = bbua_data[bbua_data['month'].isin([7, 8, 9, 10])]\n",
        "final_filtered_list.append(bbua_data)\n",
        "\n",
        "# กรองสำหรับสถานี YOM012 (เดือน 11, 12 เท่านั้น)\n",
        "yom012_data = base_filtered_df[base_filtered_df['Node'] == 'YOM012'].copy()\n",
        "yom012_data = yom012_data[yom012_data['month'].isin([11, 12])]\n",
        "final_filtered_list.append(yom012_data)\n",
        "\n",
        "# รวม DataFrame ที่ถูกกรองเฉพาะเข้าด้วยกัน\n",
        "filtered_final_output_df = pd.concat(final_filtered_list, ignore_index=True)\n",
        "\n",
        "\n",
        "print(\"DataFrame 'final_output_df' filtered to desired stations and time periods.\")\n",
        "# 4. แสดง 5 แถวแรกของ filtered_final_output_df เพื่อตรวจสอบผลลัพธ์\n",
        "print(\"5 แถวแรกของ filtered_final_output_df:\\n\")\n",
        "print(filtered_final_output_df.head())\n",
        "\n",
        "# แสดงข้อมูลสรุปเพื่อยืนยันว่าการกรองถูกต้อง\n",
        "print(\"\\nข้อมูลสรุปของ filtered_final_output_df หลังการกรองเพิ่มเติม:\")\n",
        "print(filtered_final_output_df.info())"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame 'final_output_df' filtered to desired stations and time periods.\n",
            "5 แถวแรกของ filtered_final_output_df:\n",
            "\n",
            "   Node   rain  month  years\n",
            "0  ACRU   83.0      7   2025\n",
            "1  ACRU  164.6      8   2025\n",
            "2  ACRU  223.8      9   2025\n",
            "3  ACRU   72.0     10   2025\n",
            "4  ACRU   53.6     11   2025\n",
            "\n",
            "ข้อมูลสรุปของ filtered_final_output_df หลังการกรองเพิ่มเติม:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18 entries, 0 to 17\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Node    18 non-null     object \n",
            " 1   rain    18 non-null     float64\n",
            " 2   month   18 non-null     int32  \n",
            " 3   years   18 non-null     int32  \n",
            "dtypes: float64(1), int32(2), object(1)\n",
            "memory usage: 564.0+ bytes\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c956ac1"
      },
      "source": [
        "## Save Filtered Monthly Rainfall Output\n",
        "\n",
        "### Subtask:\n",
        "บันทึกผลลัพธ์ข้อมูลปริมาณน้ำฝนรายเดือนที่ได้รับการกรองและจัดรูปแบบแล้ว (`filtered_final_output_df`) ลงในไฟล์ CSV ใหม่ชื่อ 'Y_montly_rain_hii_operation.csv' พร้อมเตรียมลิงก์ Google Drive ของไฟล์นี้ที่เปิดเป็นสาธารณะ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081cd8e9"
      },
      "source": [
        "**Reasoning**:\n",
        "I will save the `filtered_final_output_df` to the specified CSV file path without including the index, verify its existence, and then provide clear manual instructions for generating a public Google Drive link, similar to the previous saving step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39804993",
        "outputId": "4fd8f3b0-f463-44ae-ce77-5d802426219b"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. บันทึก DataFrame `filtered_final_output_df` เป็นไฟล์ CSV\n",
        "# output_monthly_rainfall_file is defined as '/content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
        "filtered_final_output_df.to_csv(output_monthly_rainfall_file, index=False)\n",
        "\n",
        "print(f\"ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ได้ถูกบันทึกที่: {output_monthly_rainfall_file}\")\n",
        "\n",
        "# 2. ตรวจสอบว่าไฟล์ถูกบันทึกสำเร็จ\n",
        "if os.path.exists(output_monthly_rainfall_file):\n",
        "    print(f\"ยืนยัน: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' บันทึกสำเร็จ.\")\n",
        "else:\n",
        "    print(f\"ข้อผิดพลาด: ไฟล์ '{os.path.basename(output_monthly_rainfall_file)}' ไม่พบบนไดรฟ์.\")\n",
        "\n",
        "# 3. สร้างลิงก์ Google Drive สาธารณะสำหรับไฟล์ (ให้คำแนะนำ)\n",
        "print(\"\\n**คำแนะนำสำหรับสร้างลิงก์ Google Drive สาธารณะ:**\")\n",
        "print(\"เนื่องจากปัญหาทางเทคนิคในการสร้างลิงก์สาธารณะด้วยโค้ดโดยตรงใน Google Colab\")\n",
        "print(\"โปรดทำตามขั้นตอนต่อไปนี้เพื่อสร้างลิงก์ด้วยตนเอง:\")\n",
        "print(\"1. ไปที่ Google Drive ของคุณ: https://drive.google.com/\")\n",
        "print(f\"2. ค้นหาไฟล์ชื่อ: '{os.path.basename(output_monthly_rainfall_file)}' ที่พาธ: '{output_monthly_rainfall_file.replace('/content/drive', '')}'\")\n",
        "print(\"3. คลิกขวาที่ไฟล์ > 'รับลิงก์' (Get link)\")\n",
        "print(\"4. เปลี่ยน 'จำกัด' (Restricted) เป็น 'ทุกคนที่มีลิงก์' (Anyone with the link)\")\n",
        "print(\"5. คัดลอกลิงก์ที่ได้มาและนำไปใช้งาน\")\n",
        "\n",
        "# Provide the expected location of the file in Google Drive for easy access\n",
        "file_location_in_drive = output_monthly_rainfall_file.replace('/content/drive/MyDrive/', 'My Drive/')\n",
        "print(f\"\\nไฟล์ของคุณจะอยู่ที่: {file_location_in_drive}\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ไฟล์ 'Y_montly_rain_hii_operation.csv' ได้ถูกบันทึกที่: /content/drive/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n",
            "ยืนยัน: ไฟล์ 'Y_montly_rain_hii_operation.csv' บันทึกสำเร็จ.\n",
            "\n",
            "**คำแนะนำสำหรับสร้างลิงก์ Google Drive สาธารณะ:**\n",
            "เนื่องจากปัญหาทางเทคนิคในการสร้างลิงก์สาธารณะด้วยโค้ดโดยตรงใน Google Colab\n",
            "โปรดทำตามขั้นตอนต่อไปนี้เพื่อสร้างลิงก์ด้วยตนเอง:\n",
            "1. ไปที่ Google Drive ของคุณ: https://drive.google.com/\n",
            "2. ค้นหาไฟล์ชื่อ: 'Y_montly_rain_hii_operation.csv' ที่พาธ: '/MyDrive/Data mining/midterm/Y_montly_rain_hii_operation.csv'\n",
            "3. คลิกขวาที่ไฟล์ > 'รับลิงก์' (Get link)\n",
            "4. เปลี่ยน 'จำกัด' (Restricted) เป็น 'ทุกคนที่มีลิงก์' (Anyone with the link)\n",
            "5. คัดลอกลิงก์ที่ได้มาและนำไปใช้งาน\n",
            "\n",
            "ไฟล์ของคุณจะอยู่ที่: My Drive/Data mining/midterm/Y_montly_rain_hii_operation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ede26337"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "จัดเตรียมไฟล์ Colab Notebook (.ipynb) ที่มีโค้ดและคำอธิบายครบถ้วนตามที่ร้องขอ และแจ้งลิงก์ Google Drive ของไฟล์ผลลัพธ์ข้อมูลรายเดือน 'Y_montly_rain_hii_operation.csv' ที่เปิดเป็นสาธารณะ พร้อมบันทึกการใช้ AI (prompt, ผลลัพธ์, การปรับแก้, เหตุผล) เพื่อส่งมอบงาน\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ec8c6e4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `final_output_df` was successfully filtered to include only the requested rainfall stations: 'ACRU', 'BBHN', 'BBUA', and 'YOM012'.\n",
        "*   The filtered monthly rainfall data was successfully saved as a CSV file named 'Y\\_montly\\_rain\\_hii\\_operation.csv' to the specified Google Drive path: `/content/drive/MyDrive/Data mining/midterm/`.\n",
        "*   The file saving operation was confirmed, ensuring the data is available in the user's Google Drive.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The user needs to manually generate a public Google Drive link for the saved 'Y\\_montly\\_rain\\_hii\\_operation.csv' file by navigating to the specified path in Google Drive, changing sharing permissions to \"Anyone with the link,\" and copying the URL.\n",
        "*   The Colab Notebook is now ready with the processed data and saving functionality, fulfilling the requirements for the final submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ลิ้งส่งงาน\n",
        "ไฟล์ผลลัพธ์ข้อมูล csv [ไฟล์ผลลัพธ์ข้อมูล csv](https://drive.google.com/file/d/1O8TZpu5CCYygXzWYLv0p8LakEW8eGAbN/view?usp=sharing)\n",
        "\n",
        "บันทึกการใช้ AI"
      ],
      "metadata": {
        "id": "jJes0hz60qTL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNhNfv2g0p4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}